{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "for p in sys.path:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "#import mapbox_vector_tile\n",
    "from time import time\n",
    "import operator\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from shapely import geometry \n",
    "from PIL import Image, ImageDraw\n",
    "from simplification.cutil import (\n",
    "    simplify_coords,\n",
    "    simplify_coords_idx,\n",
    "    simplify_coords_vw,\n",
    "    simplify_coords_vw_idx,\n",
    "    simplify_coords_vwp,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection(db_file):\n",
    "    \"\"\" create a database connection to a SQLite database \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        print(conn)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    \n",
    "    return conn\n",
    "\n",
    "def PolyArea(x,y):\n",
    "    return 0.5*np.abs(np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)))\n",
    "\n",
    "def ScoreFormula(old_number_of_datapoints, new_number_of_datapoints, processing_time):\n",
    "    return (1 - (new_number_of_datapoints / old_number_of_datapoints)) * (1 - processing_time)\n",
    "\n",
    "\n",
    "def ScaleFactor(all_geometries):\n",
    "    b_list = []\n",
    "    \n",
    "    for geometries in all_geometries:\n",
    "        \n",
    "        polygon = geometry.Polygon(geometries)\n",
    "        centroid = np.array(polygon.centroid)\n",
    "        coordinates = np.vstack(geometries)\n",
    "        \n",
    "        b = coordinates - centroid\n",
    "        b_min = np.min(b)\n",
    "        b_max = np.max(b)\n",
    "        b_list.append(b_min)\n",
    "        b_list.append(b_max)\n",
    "        \n",
    "    return np.std(b_list)\n",
    "    \n",
    "def Normalize_Geometry(coordinates1, scale_factor):\n",
    "    polygon = geometry.Polygon(coordinates1)\n",
    "    centroid = np.array(polygon.centroid)\n",
    "    coordinates2 = np.vstack(coordinates1)\n",
    "    \n",
    "    return (coordinates2 - centroid) / scale_factor\n",
    "\n",
    "def Add_One_Hot(normalized_geometry):\n",
    "    normalized_geometry = np.insert(normalized_geometry, 2, 1, axis=1)\n",
    "    normalized_geometry = np.insert(normalized_geometry, 3, 0, axis=1)\n",
    "    normalized_geometry = np.insert(normalized_geometry, 4, 0, axis=1)\n",
    "    normalized_geometry[len(normalized_geometry)-1,2] = 0\n",
    "    normalized_geometry[len(normalized_geometry)-1,4] = 1\n",
    "    \n",
    "    return normalized_geometry\n",
    "\n",
    "def Add_Zero_Padding(one_hotted_geometry, max_length):\n",
    "    boundary = max_length - len(one_hotted_geometry)\n",
    "    zero_matrix = np.zeros([boundary,5])\n",
    "    return np.append(one_hotted_geometry, zero_matrix, axis=0)\n",
    "\n",
    "def CreateGrid(poly, dx, dy):\n",
    "    \n",
    "    x_ls = []\n",
    "    y_ls = []\n",
    "\n",
    "    for a in poly:\n",
    "        x_ls.append(a[0])\n",
    "    for a in poly:\n",
    "        y_ls.append(a[1])\n",
    "        \n",
    "    minx = min(x_ls)\n",
    "    maxx = max(x_ls)\n",
    "    miny = min(y_ls)\n",
    "    maxy = max(y_ls)\n",
    "\n",
    "    nx = int(math.ceil(abs(maxx - minx)/dx))\n",
    "    ny = int(math.ceil(abs(maxy - miny)/dy))\n",
    "\n",
    "    grid = []       \n",
    "    for i in range(ny):   \n",
    "        grid.append(geometry.LineString([[minx,max(maxy-dy*i,miny)], [maxx, max(maxy-dy*i,miny)]]))\n",
    "\n",
    "    for j in range(nx):\n",
    "        grid.append(geometry.LineString([[min(minx+dx*j,maxx), maxy], [min(minx+dx*j,maxx), miny]]))\n",
    "    \n",
    "    return grid\n",
    "    \n",
    "def CheckSameIntersections(poly, simplified_coords, grid, ROUNDING):\n",
    "    \n",
    "    original = geometry.Polygon(poly)\n",
    "    simplified = geometry.Polygon(simplified_coords)\n",
    "\n",
    "    o_ls = []\n",
    "    s_ls = []\n",
    "    for line in grid:\n",
    "        x = original.intersection(line)\n",
    "        y = simplified.intersection(line)\n",
    "        if x:\n",
    "            if x.geom_type == 'Point':\n",
    "                o_ls.append(hash(tuple([round(x.coords[0][0],ROUNDING), round(x.coords[0][1],ROUNDING)])))\n",
    "            if x.geom_type == 'LineString':\n",
    "                for xy in x.coords:\n",
    "                    o_ls.append(hash(tuple([round(xy[0],ROUNDING), round(xy[1],ROUNDING)])))\n",
    "    \n",
    "        if y:\n",
    "            if y.geom_type == 'Point':\n",
    "                s_ls.append(hash(tuple([round(y.coords[0][0],ROUNDING), round(y.coords[0][1],ROUNDING)])))\n",
    "            if y.geom_type == 'LineString':\n",
    "                for xy in y.coords:\n",
    "                    s_ls.append(hash(tuple([round(xy[0],ROUNDING), round(xy[1],ROUNDING)])))\n",
    "        \n",
    "    return len(list(set(o_ls).intersection(s_ls))) / len(set(o_ls))\n",
    "\n",
    "    \n",
    "def alter_by_zoom(poly, zoom):\n",
    "\n",
    "    mpp = {\n",
    "    '0' : 156543,\n",
    "    '1' : 78271.5,\n",
    "    '2' : 39135.8,\n",
    "    '3' : 19567.88,\n",
    "    '4' : 9783.94,\n",
    "    '5' : 4891.97,\n",
    "    '6' : 2445.98,\n",
    "    '7' : 1222.99,\n",
    "    '8' : 611.5,\n",
    "    '9' : 305.75,\n",
    "    '10' : 152.87,\n",
    "    '11' : 76.44,\n",
    "    '12' : 38.219,\n",
    "    '13' : 19.109,\n",
    "    '14' : 9.555,\n",
    "    '15' : 4.777,\n",
    "    '16' : 2.3887,\n",
    "    '17' : 1.1943,\n",
    "    '18' : 0.5972,\n",
    "    '19' : 0.2986,\n",
    "    '20' : 0.14929,\n",
    "    '21' : 0.074646,\n",
    "    '22' : 0.037323\n",
    "    }\n",
    "    return (np.array(poly) / mpp[str(zoom)]).tolist()\n",
    "\n",
    "\n",
    "def check_pixel_similarity(original_coords, simplified_coords, zoom):\n",
    "    \n",
    "    poly1 = alter_by_zoom(original_coords, zoom)\n",
    "    poly2 = alter_by_zoom(simplified_coords, zoom)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for a in poly1:\n",
    "        x.append(a[0])\n",
    "        y.append(a[1])\n",
    "    \n",
    "    for a in poly1:\n",
    "        a[0] = a[0] - min(x)\n",
    "        a[1] = a[1] - min(y)\n",
    "    \n",
    "    for a in poly2:\n",
    "        a[0] = a[0] - min(x)\n",
    "        a[1] = a[1] - min(y)\n",
    "    \n",
    "    width = int(max(x) - min(x))\n",
    "    height = int(max(y) - min(y))\n",
    "\n",
    "    poly1 = [tuple(x) for x in poly1]\n",
    "    poly2 = [tuple(x) for x in poly2]\n",
    "\n",
    "    img1 = Image.new('L', (width, height), 0)\n",
    "    ImageDraw.Draw(img1).polygon(poly1, outline=1, fill=0)\n",
    "    mask1 = np.array(img1)\n",
    "    \n",
    "    img2 = Image.new('L', (width, height), 0)\n",
    "    ImageDraw.Draw(img2).polygon(poly2, outline=1, fill=0)\n",
    "    mask2 = np.array(img2)\n",
    "    \n",
    "    return np.sum(mask1 == mask2) / (width*height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pand Centrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_pand_centrum = create_connection(\"/Users/davemeijdam/Documents/Data Science/Master/Master Thesis/Data/SQLite/Pand_26116_centrum.db\")\n",
    "\n",
    "cur = conn_pand_centrum.cursor()\n",
    "cur.execute(\"SELECT data FROM tiles;\")\n",
    "\n",
    "rows = cur.fetchall()\n",
    "pand_centrum_data = []\n",
    "for row in rows:\n",
    "    pand_centrum_data.append(mapbox_vector_tile.decode(row[0]))\n",
    "    #print(row[0])\n",
    "print(len(pand_centrum_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wegdeel Buiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_wegdeel_buiten = create_connection(\"/Users/davemeijdam/Documents/Data Science/Master/Master Thesis/Data/SQLite/Wegdeel_23770_buitengebied.db\")\n",
    "\n",
    "cur = conn_wegdeel_buiten.cursor()\n",
    "cur.execute(\"SELECT data FROM tiles;\")\n",
    "\n",
    "rows = cur.fetchall()\n",
    "wegdeel_buiten_data = []\n",
    "for row in rows:\n",
    "    wegdeel_buiten_data.append(mapbox_vector_tile.decode(row[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waterdeel_export_stedelijk_geometrie.json\n",
      "wegdeel_export_buitengebied_geometrie.json\n",
      "bag_pand_buitengebeid_export_geometrie.json\n",
      "wegdeel_export_stedelijk_geometrie.json\n",
      "spoor_export_stedelijk_geometrie.json\n",
      "waterdeel_export_buitengebied_geometrie.json\n",
      "bag_pand_stedelijk_export_geometrie.json\n",
      "spoor_export_buitengebied_geometrie.json\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/davemeijdam/Documents/Data Science/Master/Master Thesis/Data/Sample_data_03_05/'\n",
    "Polygons = []\n",
    "Types = []\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    if \"geometrie.\" in filename:\n",
    "        print(filename)\n",
    "        \n",
    "        f = open(str(path + filename))\n",
    "        jsondata = json.load(f)\n",
    "        \n",
    "        \n",
    "\n",
    "        for a in jsondata['features']:\n",
    "            if len(a['geometry']['coordinates']) == 1:\n",
    "                Polygons.append(a['geometry']['coordinates'][0])\n",
    "                Types.append(a['geometry']['type'])\n",
    "            if a['geometry']['type'] == 'LineString':\n",
    "                Polygons.append(a['geometry']['coordinates'])\n",
    "                Types.append(a['geometry']['type'])\n",
    "            else:\n",
    "                for b in a['geometry']['coordinates']:\n",
    "                    Polygons.append(b)\n",
    "                    Types.append(a['geometry']['type'])\n",
    "            \n",
    "geometry_df = pd.DataFrame({'geometry':Polygons,\n",
    "                            'type':Types})\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#f = open('/Users/davemeijdam/Documents/Data Science/Master/Master Thesis/Data/Sample_data_03_05/spoor_export_buitengebied_geometrie.json')\n",
    "#wegdeeljson = json.load(f)\n",
    "#wegdeeljson\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wegdeeljson['features'][:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wegdeeljson['features'][0]['geometry']['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely.geometry as sg\n",
    "import shapely.ops as so\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ls = []\n",
    "#for a in wegdeeljson['features'][:5]:\n",
    "#    ls.append(geometry.Polygon(a['geometry']['coordinates'][0]))\n",
    "\n",
    "new_shape = so.cascaded_union(ls)\n",
    "fig, axs = plt.subplots()\n",
    "axs.set_aspect('equal', 'datalim')\n",
    "\n",
    "for geom in new_shape.geoms:    \n",
    "    xs, ys = geom.exterior.xy    \n",
    "    axs.fill(xs, ys, alpha=1, fc='r', ec='none')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely.geometry as sg\n",
    "import shapely.ops as so\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "ls = []\n",
    "for element in wegdeel_buiten_data[3]['wegdeel.se_fld15_vlakgeometrie2d']['features']:\n",
    "    \n",
    "    #print(element['geometry']['coordinates'][0])\n",
    "    #geometry.Polygon(element['geometry']['coordinates'][0])\n",
    "    element2 = element['geometry']\n",
    "    \n",
    "    if element2['type'] == 'MultiPolygon':\n",
    "        if element2['coordinates']:\n",
    "            for poly in element2['coordinates'][0]:\n",
    "                print(poly)\n",
    "                ls.append(geometry.Polygon(poly))\n",
    "    \n",
    "    else:\n",
    "        ls.append(geometry.Polygon(element['geometry']['coordinates'][0]))\n",
    "\n",
    "#r1 = sg.Polygon([[243, 2760], [242, 2760], [242, 2761], [243, 2760]])\n",
    "#r2 = sg.Polygon([[243, 2759], [243, 2760], [244, 2760], [244, 2759], [243, 2759]])\n",
    "#r3 = sg.Polygon([[244, 2759], [243, 2759], [243, 2760], [244, 2760], [244, 2759]])\n",
    "#r4 = sg.Polygon([[243, 2759], [242, 2759], [242, 2760], [243, 2760], [243, 2759]])\n",
    "#r5 = sg.Polygon([[241, 2759], [241, 2760], [242, 2759], [241, 2759]])\n",
    "\n",
    "new_shape = so.cascaded_union(ls)\n",
    "fig, axs = plt.subplots()\n",
    "axs.set_aspect('equal', 'datalim')\n",
    "\n",
    "for geom in new_shape.geoms:    \n",
    "    xs, ys = geom.exterior.xy    \n",
    "    axs.fill(xs, ys, alpha=1, fc='r', ec='none')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplification Possibilities\n",
    "simplify_possibilities = [['D-P', 0], ['D-P', 0.5], ['D-P', 0.1], ['D-P', 0.05], ['D-P', 0.01], ['D-P', 0.005], \n",
    "                          ['D-P', 0.001], ['V-W', 0.5], ['V-W', 0.1], ['V-W', 0.05], ['V-W', 0.01], \n",
    "                          ['V-W', 0.005], ['V-W', 0.001], ['V-W', 0.0005], ['V-W', 0.0001], ['V-W', 0.00005]]\n",
    "\n",
    "# Polygon length evaluation\n",
    "MAX_LENGTH_DEFICIT = -0.1\n",
    "\n",
    "# Grid\n",
    "dx = 1\n",
    "dy = 1\n",
    "ROUNDING = 1\n",
    "\n",
    "MIN_INTERSECTIONS_PERC = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lines = []\n",
    "Polygons = []\n",
    "MultiPolygons = []\n",
    "a=0\n",
    "for row in pand_centrum_data[:10000]:\n",
    "    print(str(a) + \" / \" + str(len(pand_centrum_data)), end=\"\\r\")\n",
    "    a = a + 1\n",
    "    keys = row.keys()\n",
    "    \n",
    "    for key in keys:\n",
    "        for element in row[key]['features']:\n",
    "            \n",
    "            if element['geometry']['type'] == 'LineString': \n",
    "                Lines.append(element['geometry']['coordinates'])\n",
    "            \n",
    "            if element['geometry']['type'] == 'Polygon':\n",
    "                Polygons.append(element['geometry']['coordinates'][0])\n",
    "                \n",
    "            #if element['geometry']['type'] == 'MultiPolygon':\n",
    "                #MultiPolygons.append(element['geometry']['coordinates'])\n",
    "    \n",
    "    \n",
    "\n",
    "#test = lvl10_data[0]['spoor.se_fld12_lijngeometrie2d']['features'][0]['geometry']['coordinates']\n",
    "#print(Polygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lines = []\n",
    "Polygons = []\n",
    "MultiPolygons = []\n",
    "a=0\n",
    "for row in wegdeel_buiten_data:\n",
    "    print(str(a) + \" / \" + str(len(wegdeel_buiten_data)), end=\"\\r\")\n",
    "    a = a + 1\n",
    "    keys = row.keys()\n",
    "    \n",
    "    for key in keys:\n",
    "        for element in row[key]['features']:\n",
    "            \n",
    "            if element['geometry']['type'] == 'LineString': \n",
    "                Lines.append(element['geometry']['coordinates'])\n",
    "            \n",
    "            if element['geometry']['type'] == 'Polygon':\n",
    "                Polygons.append(element['geometry']['coordinates'][0])\n",
    "                \n",
    "            if element['geometry']['type'] == 'MultiPolygon':\n",
    "                if element['geometry']['coordinates']:\n",
    "                    for poly in element['geometry']['coordinates'][0]:\n",
    "                        MultiPolygons.append(poly)\n",
    "    \n",
    "    \n",
    "\n",
    "#test = lvl10_data[0]['spoor.se_fld12_lijngeometrie2d']['features'][0]['geometry']['coordinates']\n",
    "#print(Polygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(Lines))\n",
    "print(len(Polygons))\n",
    "#print(len(MultiPolygons))\n",
    "\n",
    "ls = []\n",
    "for a in Polygons:\n",
    "    ls.append(len(a))\n",
    "    \n",
    "pd.DataFrame({'lengths':Counter(ls).keys(),\n",
    "              'freq':Counter(ls).values()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303244"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Polygons = list(geometry_df['geometry'][geometry_df['type'] == 'Polygon'])\n",
    "len(Polygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale Factor done\n",
      "Sorted the Polygons\n",
      "1304 / 25000\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/infoviz/lib/python3.7/site-packages/ipykernel_launcher.py:173: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/opt/anaconda3/envs/infoviz/lib/python3.7/site-packages/ipykernel_launcher.py:173: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24999 / 25000\r"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "length_list = []\n",
    "Polygons_sample = random.sample(Polygons, 25000)\n",
    "scale_factor = ScaleFactor(Polygons_sample)\n",
    "print(\"Scale Factor done\")\n",
    "\n",
    "\n",
    "# Decide order from longest polygon to smallest polygon\n",
    "for row in Polygons_sample:\n",
    "\n",
    "    length_list.append([row, len(row)])\n",
    "\n",
    "length_list.sort(key=operator.itemgetter(1), reverse=True)\n",
    "print(\"Sorted the Polygons\")\n",
    "a=0\n",
    "for element in length_list:\n",
    "    print(str(a) + \" / \" + str(len(length_list)), end=\"\\r\")\n",
    "    a = a + 1\n",
    "    results_dict = {}\n",
    "    poly1 = geometry.Polygon(element[0])\n",
    "    results = []\n",
    "    #grid = CreateGrid(element[0], dx, dy)\n",
    "    \n",
    "    for possibility in simplify_possibilities:\n",
    "        \n",
    "\n",
    "        if possibility[0] == 'D-P':\n",
    "            # Simplification function Douglas-Peucker\n",
    "            time_start = time()\n",
    "            simplified_coordinates = simplify_coords(element[0], possibility[1])\n",
    "            time_end = time()\n",
    "            process_time = time_end - time_start\n",
    "\n",
    "        if possibility[0] == 'V-W':\n",
    "            # Simplification function Visvalingam-Whyatt\n",
    "            time_start = time()\n",
    "            simplified_coordinates = simplify_coords_vw(element[0], possibility[1])\n",
    "            time_end = time()\n",
    "            process_time = time_end - time_start\n",
    "        \n",
    "        \n",
    "        if len(simplified_coordinates) >= 3:\n",
    "            poly2 = geometry.Polygon(simplified_coordinates)\n",
    "            #length_deficit = (poly2.length - poly1.length) / poly1.length\n",
    "        \n",
    "            # If the length deficit of the polygon is smaller(greater) than the provided MAX_LENGTH_DEFICIT, \n",
    "            # the score gets saved\n",
    "            #if length_deficit > MAX_LENGTH_DEFICIT:\n",
    "            \n",
    "            #if length_deficit == 0:\n",
    "            #    score = ScoreFormula(len(element[0]), len(simplified_coordinates), process_time)\n",
    "            #    results.append(score)\n",
    "            #    continue\n",
    "                \n",
    "            #try:\n",
    "            #    if CheckSameIntersections(element[0], simplified_coordinates, grid, ROUNDING) > MIN_INTERSECTIONS_PERC:\n",
    "            #        score = ScoreFormula(len(element[0]), len(simplified_coordinates), process_time)\n",
    "            #        results.append(score)\n",
    "            #except Exception:\n",
    "            #    continue\n",
    "            \n",
    "            if np.isnan(check_pixel_similarity(element[0], simplified_coordinates, 17)) == True:\n",
    "                results.append('Remove')\n",
    "                break\n",
    "                \n",
    "                \n",
    "            if check_pixel_similarity(element[0], simplified_coordinates, 17) == 1:\n",
    "                score = ScoreFormula(len(element[0]), len(simplified_coordinates), process_time)\n",
    "                results.append(score)\n",
    "                \n",
    "    \n",
    "    results_dict['polygon'] = Add_Zero_Padding(Add_One_Hot(Normalize_Geometry(element[0], scale_factor)), len(length_list[0][0]))\n",
    "    if results[0] == 'Remove':\n",
    "        results_dict['algorithm'] = len(simplify_possibilities)\n",
    "        \n",
    "    else:    \n",
    "        results_dict['algorithm'] = results.index(max(results))\n",
    "        \n",
    "    results_list.append(results_dict)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "a = 0\n",
    "for element in results_list:\n",
    "    print(str(a) + \" / \" + str(len(length_list)), end=\"\\r\")\n",
    "    a = a + 1\n",
    "    \n",
    "    X.append(element['polygon'])\n",
    "    y.append(element['algorithm'])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "# Calculate the deficit in the number of \n",
    "#point_deficit = len(coordinates) - len(simplified_coordinates)\n",
    "#print('Point Deficit: ' + str(point_deficit) + ' out of ' + str(len(coordinates)))\n",
    "\n",
    "#old_area = PolyArea(old_xs,old_ys)\n",
    "#new_area = PolyArea(new_xs,new_ys)\n",
    "#area_deficit_percentage = (new_area - old_area) / old_area\n",
    "#print(area_deficit_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(length_list[0][0]))\n",
    "time_start = time()\n",
    "Add_Zero_Padding(Add_One_Hot(Normalize_Geometry(length_list[0][0], scale_factor)), len(length_list[0][0]))\n",
    "time_end = time()\n",
    "print(time_end - time_start)\n",
    "\n",
    "time_start = time()\n",
    "CheckSameIntersections(length_list[0][0], simplified_coordinates, grid, ROUNDING)\n",
    "time_end = time()\n",
    "print(time_end - time_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keys</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>14718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    keys   freq\n",
       "0      0  14718\n",
       "1      1   3351\n",
       "2      2   1208\n",
       "3      4   1460\n",
       "4      3    684\n",
       "5      5    841\n",
       "6      6    987\n",
       "7     16     81\n",
       "8      7    699\n",
       "9      8    347\n",
       "10    10    145\n",
       "11     9    225\n",
       "12    11    151\n",
       "13    13     25\n",
       "14    12     61\n",
       "15    14     12\n",
       "16    15      5"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'keys':list(Counter(y).keys()),\n",
    "              'freq':list(Counter(y).values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.44091214 -1.00739838  1.          0.          0.        ]\n",
      " [ 3.46945231 -0.94843036  1.          0.          0.        ]\n",
      " [ 3.47063644 -0.94028538  1.          0.          0.        ]\n",
      " ...\n",
      " [ 3.42329672 -1.06417278  1.          0.          0.        ]\n",
      " [ 3.43223643 -1.03582675  1.          0.          0.        ]\n",
      " [ 3.44091214 -1.00739838  0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select index of simplification possibility\n",
    "INDEX = 6\n",
    "\n",
    "\n",
    "possibility = simplify_possibilities[INDEX]\n",
    "\n",
    "if possibility[0] == 'D-P':\n",
    "    # Simplification function Douglas-Peucker\n",
    "    simplified_coordinates = simplify_coords(coordinates, possibility[1])\n",
    "\n",
    "if possibility[0] == 'V-W':\n",
    "    # Simplification function Visvalingam-Whyatt\n",
    "    simplified_coordinates = simplify_coords_vw(coordinates, possibility[1])\n",
    "\n",
    "old_xs, old_ys = zip(*coordinates)\n",
    "new_xs, new_ys = zip(*simplified_coordinates)\n",
    "\n",
    "print(len(simplified_coordinates))\n",
    "print(len(coordinates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(old_xs, old_ys)\n",
    "plt.plot(new_xs, new_ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1021, 5)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 1017, 32)          832       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 339, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 335, 64)           10304     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 17)                1105      \n",
      "=================================================================\n",
      "Total params: 12,241\n",
      "Trainable params: 12,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_shape = X[0].shape\n",
    "print(input_shape)\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(32, 5, activation='relu', input_shape=input_shape))\n",
    "model.add(layers.MaxPooling1D(3,3))\n",
    "\n",
    "model.add(layers.Conv1D(64, 5, activation='relu'))\n",
    "model.add(layers.GlobalAveragePooling1D())\n",
    "model.add(layers.Dense(len(simplify_possibilities)+1, activation='softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      "20000/20000 [==============================] - 61s 3ms/sample - loss: 2.3975 - accuracy: 0.4756 - val_loss: 0.7232 - val_accuracy: 0.9900\n",
      "Epoch 2/50\n",
      "20000/20000 [==============================] - 57s 3ms/sample - loss: 1.7916 - accuracy: 0.4884 - val_loss: 0.8143 - val_accuracy: 0.9900\n",
      "Epoch 3/50\n",
      "20000/20000 [==============================] - 55s 3ms/sample - loss: 1.7595 - accuracy: 0.4883 - val_loss: 0.7942 - val_accuracy: 0.9900\n",
      "Epoch 4/50\n",
      "20000/20000 [==============================] - 45s 2ms/sample - loss: 1.7494 - accuracy: 0.4877 - val_loss: 0.7341 - val_accuracy: 0.9900\n",
      "Epoch 5/50\n",
      "20000/20000 [==============================] - 53s 3ms/sample - loss: 1.7416 - accuracy: 0.4875 - val_loss: 0.7499 - val_accuracy: 0.9900\n",
      "Epoch 6/50\n",
      "20000/20000 [==============================] - 72s 4ms/sample - loss: 1.7357 - accuracy: 0.4870 - val_loss: 0.8042 - val_accuracy: 0.9900\n",
      "Epoch 7/50\n",
      "20000/20000 [==============================] - 65s 3ms/sample - loss: 1.7311 - accuracy: 0.4868 - val_loss: 0.7623 - val_accuracy: 0.9900\n",
      "Epoch 8/50\n",
      "20000/20000 [==============================] - 55s 3ms/sample - loss: 1.7275 - accuracy: 0.4854 - val_loss: 0.7362 - val_accuracy: 0.9900\n",
      "Epoch 9/50\n",
      "20000/20000 [==============================] - 57s 3ms/sample - loss: 1.7243 - accuracy: 0.4861 - val_loss: 0.7696 - val_accuracy: 0.9900\n",
      "Epoch 10/50\n",
      "20000/20000 [==============================] - 67s 3ms/sample - loss: 1.7215 - accuracy: 0.4871 - val_loss: 0.7250 - val_accuracy: 0.9900\n",
      "Epoch 11/50\n",
      "20000/20000 [==============================] - 63s 3ms/sample - loss: 1.7202 - accuracy: 0.4863 - val_loss: 0.7516 - val_accuracy: 0.9900\n",
      "Epoch 12/50\n",
      "20000/20000 [==============================] - 56s 3ms/sample - loss: 1.7174 - accuracy: 0.4861 - val_loss: 0.7558 - val_accuracy: 0.9900\n",
      "Epoch 13/50\n",
      " 1200/20000 [>.............................] - ETA: 1:05 - loss: 1.7252 - accuracy: 0.4938"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-60ebe4b9d986>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                     verbose=1)\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/infoviz/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/anaconda3/envs/infoviz/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/infoviz/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/infoviz/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/infoviz/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/infoviz/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/infoviz/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/infoviz/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/infoviz/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/infoviz/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/anaconda3/envs/infoviz/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE = 400\n",
    "EPOCHS = 50\n",
    "\n",
    "history = model.fit(X,\n",
    "                    y,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(len(X), 32, 5)\n",
    "        self.pool1 = nn.MaxPool1d(3, 3)\n",
    "        self.conv2 = nn.Conv1d(61, 64, 5)\n",
    "        self.pool2 = nn.AvgPool1d(64)\n",
    "        self.fc1 = nn.Linear(13,13)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = F.softmax(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
