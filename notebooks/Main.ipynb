{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8923376",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f38aa862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "#import mapbox_vector_tile\n",
    "from time import time\n",
    "import operator\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import argmax\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from shapely import geometry \n",
    "from PIL import Image, ImageDraw\n",
    "from simplification.cutil import (\n",
    "    simplify_coords,\n",
    "    simplify_coords_idx,\n",
    "    simplify_coords_vw,\n",
    "    simplify_coords_vw_idx,\n",
    "    simplify_coords_vwp,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fba032c",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "5f8acaff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_connection(db_file):\n",
    "    \"\"\" create a database connection to a SQLite database \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        print(conn)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    \n",
    "    return conn\n",
    "\n",
    "def PolyArea(x,y):\n",
    "    return 0.5*np.abs(np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)))\n",
    "\n",
    "def ScoreFormula(old_number_of_datapoints, new_number_of_datapoints, processing_time):\n",
    "    return (1 - (new_number_of_datapoints / old_number_of_datapoints)) * (1 - processing_time)\n",
    "\n",
    "\n",
    "def ScaleFactor(all_geometries):\n",
    "    b_list = []\n",
    "    \n",
    "    for geometries in all_geometries:\n",
    "        \n",
    "        polygon = geometry.Polygon(geometries)\n",
    "        centroid = np.array(polygon.centroid)\n",
    "        coordinates = np.vstack(geometries)\n",
    "        \n",
    "        b = coordinates - centroid\n",
    "        b_min = np.min(b)\n",
    "        b_max = np.max(b)\n",
    "        b_list.append(b_min)\n",
    "        b_list.append(b_max)\n",
    "        \n",
    "    return np.std(b_list)\n",
    "    \n",
    "def Normalize_Geometry(coordinates1, scale_factor):\n",
    "    polygon = geometry.Polygon(coordinates1)\n",
    "    centroid = np.array(polygon.centroid)\n",
    "    coordinates2 = np.vstack(coordinates1)\n",
    "    \n",
    "    return (coordinates2 - centroid) / scale_factor\n",
    "\n",
    "def Add_One_Hot(normalized_geometry):\n",
    "    normalized_geometry = np.insert(normalized_geometry, 2, 1, axis=1)\n",
    "    normalized_geometry = np.insert(normalized_geometry, 3, 0, axis=1)\n",
    "    normalized_geometry = np.insert(normalized_geometry, 4, 0, axis=1)\n",
    "    normalized_geometry[len(normalized_geometry)-1,2] = 0\n",
    "    normalized_geometry[len(normalized_geometry)-1,4] = 1\n",
    "    \n",
    "    return normalized_geometry\n",
    "\n",
    "def Add_Zero_Padding(one_hotted_geometry, max_length):\n",
    "    boundary = max_length - len(one_hotted_geometry)\n",
    "    zero_matrix = np.zeros([boundary,len(one_hotted_geometry[0])])\n",
    "    return np.append(one_hotted_geometry, zero_matrix, axis=0)\n",
    "\n",
    "def moment(xy, p, q):\n",
    "    xy = np.asarray(xy)\n",
    "    x = xy[:, 0]\n",
    "    y = xy[:, 1]\n",
    "    x = (x**p) * (x != 0)\n",
    "    y = (y**q) * (y != 0)\n",
    "    M = (x * y).sum(-1)\n",
    "    return torch.tensor(M)\n",
    "\n",
    "def c_mass(xy):\n",
    "    xy = np.asarray(xy)\n",
    "    mass = moment(xy, 0, 0)\n",
    "    mx = moment(xy, 1, 0) / mass\n",
    "    my = moment(xy, 0, 1) / mass\n",
    "    return [mx,my]\n",
    "\n",
    "def mu(xy, p, q):\n",
    "    xy = np.asarray(xy)\n",
    "    m = c_mass(xy)\n",
    "    x = xy[:, 0]\n",
    "    y = xy[:, 1]\n",
    "    x = ((x - m[0])**p) * (x != 0)\n",
    "    y = ((y - m[1])**q) * (y != 0)\n",
    "    M = (x * y).sum(-1)\n",
    "    return M\n",
    "\n",
    "def scale_factor_calculation(xy):\n",
    "    mu_list = [mu(i,0,0) for i in xy]\n",
    "    return sum(mu_list) / len(mu_list)\n",
    "        \n",
    "\n",
    "def scale_factor_apply(xy):\n",
    "    xy = torch.Tensor(xy)\n",
    "    return torch.sqrt((moment(xy,2,0) + moment(xy, 0, 2))/10000000000)\n",
    "\n",
    "def canonical_transformation(xy):\n",
    "#   translation\n",
    "    xy = torch.Tensor(xy)\n",
    "    m = torch.Tensor(c_mass(xy))\n",
    "    x = xy - m.view(1, 2) * (xy[:, 0] != 0).view(-1, 1)\n",
    "\n",
    "#   scale\n",
    "    scale = scale_factor_apply(xy)\n",
    "    x = x / scale\n",
    "    \n",
    "    \n",
    "#   rotation\n",
    "    m_20 = moment(x, 2, 0)\n",
    "    m_02 = moment(x, 0, 2)\n",
    "    m_11 = moment(x, 1, 1)\n",
    "    \n",
    "    angle = np.arctan2(2 * m_11, m_20 - m_02) / 2.0\n",
    "#     return angle\n",
    "    if angle < 0:\n",
    "        angle = np.pi + angle # this is a bad solution\n",
    "        # we need to analyze m_30, m_21, m_12, m_03 to check for flip symmetry\n",
    "    #print(angle*180/np.pi)\n",
    "    \n",
    "    M = torch.Tensor([\n",
    "        [np.cos(angle), np.sin(angle)],\n",
    "        [np.sin(-angle), np.cos(angle)]\n",
    "    ])\n",
    "    x = (M @ x.T).T[None]\n",
    "    \n",
    "    \n",
    "    return x\n",
    "\n",
    "def getAngle(a, b, c):\n",
    "    ang = math.degrees(math.atan2(c[1]-b[1], c[0]-b[0]) - math.atan2(a[1]-b[1], a[0]-b[0]))\n",
    "    return ang + 360 if ang < 0 else ang\n",
    "\n",
    "def polygon_properties(xy):\n",
    "    length = geometry.Polygon(xy).length\n",
    "    points = len(xy)\n",
    "    \n",
    "    b=1\n",
    "    points_distance = []\n",
    "    for coord in xy[:-2]:\n",
    "        points_distance.append(geometry.LineString([coord,xy[b]]).length)\n",
    "        b += 1 \n",
    "    points_distance = pd.DataFrame(points_distance)\n",
    "        \n",
    "    b=1\n",
    "    c=2\n",
    "    angles = []\n",
    "    for coord in xy[:-3]:\n",
    "        angles.append(getAngle(coord, xy[b], xy[c]))\n",
    "        b+=1\n",
    "        c+=1\n",
    "    angles = pd.DataFrame(angles)\n",
    "    \n",
    "    # [number of points, length, average PD, std PD, min PD, max PD, average angle, std angle, min angle, max angle]\n",
    "    return [points, length, points_distance.describe()[0][1], points_distance.describe()[0][2],\n",
    "           points_distance.describe()[0][3], points_distance.describe()[0][7], angles.describe()[0][1], \n",
    "            angles.describe()[0][2], angles.describe()[0][3], angles.describe()[0][7]]\n",
    "\n",
    "ScoreFormula(50,25,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3130d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateGrid(poly, dx, dy):\n",
    "    \n",
    "    x_ls = []\n",
    "    y_ls = []\n",
    "\n",
    "    for a in poly:\n",
    "        x_ls.append(a[0])\n",
    "    for a in poly:\n",
    "        y_ls.append(a[1])\n",
    "        \n",
    "    minx = min(x_ls)\n",
    "    maxx = max(x_ls)\n",
    "    miny = min(y_ls)\n",
    "    maxy = max(y_ls)\n",
    "\n",
    "    nx = int(math.ceil(abs(maxx - minx)/dx))\n",
    "    ny = int(math.ceil(abs(maxy - miny)/dy))\n",
    "\n",
    "    grid = []       \n",
    "    for i in range(ny):   \n",
    "        grid.append(geometry.LineString([[minx,max(maxy-dy*i,miny)], [maxx, max(maxy-dy*i,miny)]]))\n",
    "\n",
    "    for j in range(nx):\n",
    "        grid.append(geometry.LineString([[min(minx+dx*j,maxx), maxy], [min(minx+dx*j,maxx), miny]]))\n",
    "    \n",
    "    return grid\n",
    "    \n",
    "def CheckSameIntersections(poly, simplified_coords, grid, ROUNDING):\n",
    "    \n",
    "    original = geometry.Polygon(poly)\n",
    "    simplified = geometry.Polygon(simplified_coords)\n",
    "\n",
    "    o_ls = []\n",
    "    s_ls = []\n",
    "    for line in grid:\n",
    "        x = original.intersection(line)\n",
    "        y = simplified.intersection(line)\n",
    "        if x:\n",
    "            if x.geom_type == 'Point':\n",
    "                o_ls.append(hash(tuple([round(x.coords[0][0],ROUNDING), round(x.coords[0][1],ROUNDING)])))\n",
    "            if x.geom_type == 'LineString':\n",
    "                for xy in x.coords:\n",
    "                    o_ls.append(hash(tuple([round(xy[0],ROUNDING), round(xy[1],ROUNDING)])))\n",
    "    \n",
    "        if y:\n",
    "            if y.geom_type == 'Point':\n",
    "                s_ls.append(hash(tuple([round(y.coords[0][0],ROUNDING), round(y.coords[0][1],ROUNDING)])))\n",
    "            if y.geom_type == 'LineString':\n",
    "                for xy in y.coords:\n",
    "                    s_ls.append(hash(tuple([round(xy[0],ROUNDING), round(xy[1],ROUNDING)])))\n",
    "        \n",
    "    return len(list(set(o_ls).intersection(s_ls))) / len(set(o_ls))\n",
    "\n",
    "    \n",
    "def alter_by_zoom(poly, zoom):\n",
    "\n",
    "    mpp = {\n",
    "    '0' : 156543,\n",
    "    '1' : 78271.5,\n",
    "    '2' : 39135.8,\n",
    "    '3' : 19567.88,\n",
    "    '4' : 9783.94,\n",
    "    '5' : 4891.97,\n",
    "    '6' : 2445.98,\n",
    "    '7' : 1222.99,\n",
    "    '8' : 611.5,\n",
    "    '9' : 305.75,\n",
    "    '10' : 152.87,\n",
    "    '11' : 76.44,\n",
    "    '12' : 38.219,\n",
    "    '13' : 19.109,\n",
    "    '14' : 9.555,\n",
    "    '15' : 4.777,\n",
    "    '16' : 2.3887,\n",
    "    '17' : 1.1943,\n",
    "    '18' : 0.5972,\n",
    "    '19' : 0.2986,\n",
    "    '20' : 0.14929,\n",
    "    '21' : 0.074646,\n",
    "    '22' : 0.037323\n",
    "    }\n",
    "    return (np.array(poly) / mpp[str(zoom)]).tolist()\n",
    "\n",
    "\n",
    "def check_pixel_similarity(original_coords, simplified_coords, zoom):\n",
    "    \n",
    "    poly1 = alter_by_zoom(original_coords, zoom)\n",
    "    poly2 = alter_by_zoom(simplified_coords, zoom)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for a in poly1:\n",
    "        x.append(a[0])\n",
    "        y.append(a[1])\n",
    "    \n",
    "    for a in poly1:\n",
    "        a[0] = a[0] - min(x)\n",
    "        a[1] = a[1] - min(y)\n",
    "    \n",
    "    for a in poly2:\n",
    "        a[0] = a[0] - min(x)\n",
    "        a[1] = a[1] - min(y)\n",
    "    \n",
    "    width = int(max(x) - min(x))\n",
    "    height = int(max(y) - min(y))\n",
    "\n",
    "    poly1 = [tuple(x) for x in poly1]\n",
    "    poly2 = [tuple(x) for x in poly2]\n",
    "\n",
    "    img1 = Image.new('L', (width, height), 0)\n",
    "    ImageDraw.Draw(img1).polygon(poly1, outline=1, fill=0)\n",
    "    mask1 = np.array(img1)\n",
    "    \n",
    "    img2 = Image.new('L', (width, height), 0)\n",
    "    ImageDraw.Draw(img2).polygon(poly2, outline=1, fill=0)\n",
    "    mask2 = np.array(img2)\n",
    "    \n",
    "    return np.sum(mask1 == mask2) / (width*height)\n",
    "    #return mask1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eb1b8b",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "fbd72612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waterdeel_export_stedelijk_geometrie.json\n",
      "wegdeel_export_buitengebied_geometrie.json\n",
      "bag_pand_buitengebeid_export_geometrie.json\n",
      "wegdeel_export_stedelijk_geometrie.json\n",
      "spoor_export_stedelijk_geometrie.json\n",
      "waterdeel_export_buitengebied_geometrie.json\n",
      "bag_pand_stedelijk_export_geometrie.json\n",
      "spoor_export_buitengebied_geometrie.json\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/davemeijdam/Documents/Data Science/Master/Master Thesis/Data/Sample_data_03_05/'\n",
    "Polygons = []\n",
    "Types = []\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    if \"geometrie.\" in filename:\n",
    "        print(filename)\n",
    "        \n",
    "        f = open(str(path + filename))\n",
    "        jsondata = json.load(f)\n",
    "        \n",
    "        \n",
    "\n",
    "        for a in jsondata['features']:\n",
    "            if len(a['geometry']['coordinates']) == 1:\n",
    "                Polygons.append(a['geometry']['coordinates'][0])\n",
    "                Types.append(a['geometry']['type'])\n",
    "            if a['geometry']['type'] == 'LineString':\n",
    "                Polygons.append(a['geometry']['coordinates'])\n",
    "                Types.append(a['geometry']['type'])\n",
    "            else:\n",
    "                for b in a['geometry']['coordinates']:\n",
    "                    Polygons.append(b)\n",
    "                    Types.append(a['geometry']['type'])\n",
    "            \n",
    "geometry_df = pd.DataFrame({'geometry':Polygons,\n",
    "                            'type':Types})\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#f = open('/Users/davemeijdam/Documents/Data Science/Master/Master Thesis/Data/Sample_data_03_05/spoor_export_buitengebied_geometrie.json')\n",
    "#wegdeeljson = json.load(f)\n",
    "#wegdeeljson\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8911f217",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a2e8ec16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simplification Possibilities\n",
    "simplify_possibilities = [['D-P', 0], ['D-P', 0.5], ['D-P', 0.1], ['D-P', 0.05], ['D-P', 0.01], ['D-P', 0.005], \n",
    "                          ['D-P', 0.001], ['V-W', 0.5], ['V-W', 0.1], ['V-W', 0.05], ['V-W', 0.01], \n",
    "                          ['V-W', 0.005]]\n",
    "\n",
    "#simplify_possibilities = [['D-P', 0], ['D-P', 0.5], ['D-P', 0.1], ['D-P', 0.05], ['D-P', 0.01], ['D-P', 0.005], \n",
    "#                          ['D-P', 0.001], ['V-W', 0.5], ['V-W', 0.1], ['V-W', 0.05], ['V-W', 0.01], \n",
    "#                          ['V-W', 0.005], ['V-W', 0.001], ['V-W', 0.0005], ['V-W', 0.0001], ['V-W', 0.00005]]\n",
    "\n",
    "# Polygon length evaluation\n",
    "MAX_LENGTH_DEFICIT = -0.1\n",
    "\n",
    "# Grid\n",
    "dx = 1\n",
    "dy = 1\n",
    "ROUNDING = 1\n",
    "\n",
    "MIN_INTERSECTIONS_PERC = 0.75\n",
    "\n",
    "len(simplify_possibilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8fe237",
   "metadata": {},
   "source": [
    "# Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "fa2b1b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294580"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Polygons = list(geometry_df['geometry'][geometry_df['type'] == 'Polygon'])\n",
    "Lines = list(geometry_df['geometry'][geometry_df['type'] == 'LineString'])\n",
    "\n",
    "Polygons_list = []\n",
    "for element in Polygons:\n",
    "    if len(element) < 100:\n",
    "        Polygons_list.append(element)\n",
    "Polygons = Polygons_list\n",
    "len(Polygons)\n",
    "#len(Lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f83de39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale Factor done\n",
      "Sorted the Polygons\n",
      "6189 / 250000\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/infoviz/lib/python3.7/site-packages/ipykernel_launcher.py:118: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/opt/anaconda3/envs/infoviz/lib/python3.7/site-packages/ipykernel_launcher.py:118: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249999 / 250000\r"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "length_list = []\n",
    "Polygons_sample = random.sample(Polygons, 250000)\n",
    "#scale_factor = scale_factor_calculation(Polygons_sample)\n",
    "print(\"Scale Factor done\")\n",
    "\n",
    "\n",
    "# Decide order from longest polygon to smallest polygon\n",
    "for row in Polygons_sample:\n",
    "\n",
    "    length_list.append([row, len(row)])\n",
    "\n",
    "length_list.sort(key=operator.itemgetter(1), reverse=True)\n",
    "print(\"Sorted the Polygons\")\n",
    "a=0\n",
    "\n",
    "y_ls = []\n",
    "    \n",
    "for element in length_list:\n",
    "    print(str(a) + \" / \" + str(len(length_list)), end=\"\\r\")\n",
    "    a = a + 1\n",
    "    results_dict = {}\n",
    "    poly1 = geometry.Polygon(element[0])\n",
    "    results = []\n",
    "    process_time_tensor = torch.zeros(len(simplify_possibilities)+1)\n",
    "    datasize_tensor = torch.zeros(len(simplify_possibilities)+1)\n",
    "    variance_penalty_tensor = torch.ones(len(simplify_possibilities)+1)\n",
    "    \n",
    "    i=0\n",
    "    for possibility in simplify_possibilities:\n",
    "        \n",
    "\n",
    "        if possibility[0] == 'D-P':\n",
    "            # Simplification function Douglas-Peucker\n",
    "            time_start = time()\n",
    "            simplified_coordinates = simplify_coords(element[0], possibility[1])\n",
    "            time_end = time()\n",
    "            process_time = time_end - time_start\n",
    "\n",
    "        if possibility[0] == 'V-W':\n",
    "            # Simplification function Visvalingam-Whyatt\n",
    "            time_start = time()\n",
    "            simplified_coordinates = simplify_coords_vw(element[0], possibility[1])\n",
    "            time_end = time()\n",
    "            process_time = time_end - time_start\n",
    "            \n",
    "        process_time_tensor[i] = torch.tensor(process_time * 1000)\n",
    "        datasize_tensor[i] = torch.tensor(len(simplified_coordinates) / len(element[0]))\n",
    "        \n",
    "        \n",
    "        if len(simplified_coordinates) >= 3:\n",
    "            poly2 = geometry.Polygon(simplified_coordinates)\n",
    "            \n",
    "            if np.isnan(check_pixel_similarity(element[0], simplified_coordinates, 17)) == True:\n",
    "                results.append('Remove')\n",
    "                variance_penalty_tensor[len(simplify_possibilities)] = torch.tensor(0)\n",
    "                \n",
    "                \n",
    "            if check_pixel_similarity(element[0], simplified_coordinates, 17) == 1:\n",
    "                score = ScoreFormula(len(element[0]), len(simplified_coordinates), process_time)\n",
    "                #results.append(score)\n",
    "                dicti = {\"i\": i, \"score\": score}\n",
    "                results.append(dicti)\n",
    "                variance_penalty_tensor[i] = torch.tensor(0)\n",
    "        \n",
    "        \n",
    "        i = i + 1\n",
    "    y_tensor = torch.Tensor(process_time_tensor * datasize_tensor + variance_penalty_tensor)\n",
    "    y_ls.append(y_tensor)\n",
    "    \n",
    "    #results_dict['polygon'] = Add_Zero_Padding(element[0], len(length_list[0][0]))\n",
    "    results_dict['polygon'] = Add_Zero_Padding(canonical_transformation(element[0])[0], len(length_list[0][0]))\n",
    "    results_dict['properties'] = polygon_properties(element[0])\n",
    "    \n",
    "    if results[0] == 'Remove':\n",
    "        results_dict['algorithm_top1'] = len(simplify_possibilities)\n",
    "        \n",
    "    else:\n",
    "        if len(results) >= 1:\n",
    "        \n",
    "            results_df = pd.DataFrame(results).sort_values('score', ascending = False)\n",
    "            results_dict['algorithm_top1'] = results_df['i'].iloc[0]\n",
    "        \n",
    "        if len(results) >= 3:\n",
    "            results_df = pd.DataFrame(results).sort_values('score', ascending = False)\n",
    "            results_dict['algorithm_top3'] = list(results_df['i'][0:3])\n",
    "        \n",
    "        if len(results) >= 5:\n",
    "            results_df = pd.DataFrame(results).sort_values('score', ascending = False)\n",
    "            results_dict['algorithm_top5'] = list(results_df['i'][0:5])\n",
    "    \n",
    "    results_dict['algorithm_all'] = results_df['i']\n",
    "\n",
    "    results_dict['original_geom'] = element[0]\n",
    "        \n",
    "    results_list.append(results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3faf29",
   "metadata": {},
   "source": [
    "# Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a0f97692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Labels and Normalized Data\n",
    "#pickle.dump( results_list, open( \"/Users/davemeijdam/Documents/Data Science/Master/Master Thesis/Scripts/data/temp/results_list_NoNorm.p\", \"wb\" ) )\n",
    "\n",
    "#pickle.dump( results_list, open( \"/Users/davemeijdam/Documents/Data Science/Master/Master Thesis/Scripts/data/temp/results_list.p\", \"wb\" ) )\n",
    "#pickle.dump( results_list, open( \"/Users/davemeijdam/Documents/Data Science/Master/Master Thesis/Scripts/data/temp/results_list2.p\", \"wb\" ) )\n",
    "pickle.dump( results_list, open( \"/Users/davemeijdam/Documents/Data Science/Master/Master Thesis/Scripts/data/temp/results_list3.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fa0665",
   "metadata": {},
   "source": [
    "# Data Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "c1ade7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     165731\n",
       "8      17352\n",
       "7      16539\n",
       "10     11593\n",
       "9       8881\n",
       "1       8774\n",
       "11      7056\n",
       "2       4226\n",
       "3       3292\n",
       "4       2352\n",
       "6       1859\n",
       "5       1537\n",
       "12       808\n",
       "Name: algorithm_top1, dtype: int64"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_list = pickle.load( open( \"/Users/davemeijdam/Documents/Data Science/Master/Master Thesis/Scripts/data/temp/results_list3.p\", \"rb\" ) )\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "results_df['algorithm_top1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "acb71142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polygon</th>\n",
       "      <th>properties</th>\n",
       "      <th>algorithm_top1</th>\n",
       "      <th>algorithm_top3</th>\n",
       "      <th>algorithm_all</th>\n",
       "      <th>original_geom</th>\n",
       "      <th>algorithm_top5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-0.0873810425400734, 0.06564639508724213], [...</td>\n",
       "      <td>[99, 615.3124009639364, 6.2862951513134515, 11...</td>\n",
       "      <td>11</td>\n",
       "      <td>[11, 6, 0]</td>\n",
       "      <td>[11, 6, 0]</td>\n",
       "      <td>[[569212.006392023, 6817744.41890897], [569221...</td>\n",
       "      <td>[11, 6, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.0726815015077591, 0.030753642320632935], [...</td>\n",
       "      <td>[99, 621.8473441812531, 6.390577741162117, 8.7...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[[567001.947009817, 6813423.48716989], [567003...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-0.008565051481127739, -0.006880257744342089...</td>\n",
       "      <td>[99, 407.7399565199074, 4.178463394979179, 11....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[[571569.725229183, 6814491.43055043], [571568...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[-0.14224334061145782, 0.013581816107034683],...</td>\n",
       "      <td>[99, 1368.3399200611218, 13.9689776248775, 17....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[[511604.226279186, 6793164.49240385], [511606...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.0023038601502776146, 0.00545631954446435],...</td>\n",
       "      <td>[99, 587.1522959443573, 5.615763266535105, 4.7...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[[512670.53379004, 6788978.16589243], [512670....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249995</th>\n",
       "      <td>[[-0.014310438185930252, 0.0030228430405259132...</td>\n",
       "      <td>[4, 11.52625317168625, 3.343070463600278, 0.09...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]</td>\n",
       "      <td>[[567728.230127489, 6816773.19594685], [567729...</td>\n",
       "      <td>[0, 1, 2, 3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249996</th>\n",
       "      <td>[[0.010053538717329502, 0.004469005391001701],...</td>\n",
       "      <td>[4, 11.523363234321945, 4.08658696919464, 1.02...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]</td>\n",
       "      <td>[[567680.278299499, 6816931.16778507], [567681...</td>\n",
       "      <td>[0, 1, 2, 3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249997</th>\n",
       "      <td>[[-0.010315812192857265, -0.004728611558675766...</td>\n",
       "      <td>[4, 11.721705044877638, 4.141911081705699, 1.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]</td>\n",
       "      <td>[[567760.824596053, 6816701.11418839], [567759...</td>\n",
       "      <td>[0, 1, 2, 3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249998</th>\n",
       "      <td>[[0.0018060868605971336, -0.01866050437092781]...</td>\n",
       "      <td>[4, 26.49451881314417, 9.208097229569486, 2.39...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]</td>\n",
       "      <td>[[592573.660072807, 6742824.04699339], [592566...</td>\n",
       "      <td>[0, 1, 2, 3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249999</th>\n",
       "      <td>[[-0.03837365657091141, -0.024241933599114418]...</td>\n",
       "      <td>[4, 58.203787561023425, 24.42445908158193, 1.7...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]</td>\n",
       "      <td>[[517797.139952014, 6775080.54679077], [517818...</td>\n",
       "      <td>[0, 1, 2, 3, 4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  polygon  \\\n",
       "0       [[-0.0873810425400734, 0.06564639508724213], [...   \n",
       "1       [[0.0726815015077591, 0.030753642320632935], [...   \n",
       "2       [[-0.008565051481127739, -0.006880257744342089...   \n",
       "3       [[-0.14224334061145782, 0.013581816107034683],...   \n",
       "4       [[0.0023038601502776146, 0.00545631954446435],...   \n",
       "...                                                   ...   \n",
       "249995  [[-0.014310438185930252, 0.0030228430405259132...   \n",
       "249996  [[0.010053538717329502, 0.004469005391001701],...   \n",
       "249997  [[-0.010315812192857265, -0.004728611558675766...   \n",
       "249998  [[0.0018060868605971336, -0.01866050437092781]...   \n",
       "249999  [[-0.03837365657091141, -0.024241933599114418]...   \n",
       "\n",
       "                                               properties  algorithm_top1  \\\n",
       "0       [99, 615.3124009639364, 6.2862951513134515, 11...              11   \n",
       "1       [99, 621.8473441812531, 6.390577741162117, 8.7...               0   \n",
       "2       [99, 407.7399565199074, 4.178463394979179, 11....               0   \n",
       "3       [99, 1368.3399200611218, 13.9689776248775, 17....               0   \n",
       "4       [99, 587.1522959443573, 5.615763266535105, 4.7...               0   \n",
       "...                                                   ...             ...   \n",
       "249995  [4, 11.52625317168625, 3.343070463600278, 0.09...               0   \n",
       "249996  [4, 11.523363234321945, 4.08658696919464, 1.02...               0   \n",
       "249997  [4, 11.721705044877638, 4.141911081705699, 1.0...               0   \n",
       "249998  [4, 26.49451881314417, 9.208097229569486, 2.39...               0   \n",
       "249999  [4, 58.203787561023425, 24.42445908158193, 1.7...               0   \n",
       "\n",
       "       algorithm_top3                           algorithm_all  \\\n",
       "0          [11, 6, 0]                              [11, 6, 0]   \n",
       "1                   0                                     [0]   \n",
       "2                   0                                     [0]   \n",
       "3                   0                                     [0]   \n",
       "4                   0                                     [0]   \n",
       "...               ...                                     ...   \n",
       "249995      [0, 1, 2]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]   \n",
       "249996      [0, 1, 2]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]   \n",
       "249997      [0, 1, 2]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]   \n",
       "249998      [0, 1, 2]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]   \n",
       "249999      [0, 1, 2]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]   \n",
       "\n",
       "                                            original_geom   algorithm_top5  \n",
       "0       [[569212.006392023, 6817744.41890897], [569221...       [11, 6, 0]  \n",
       "1       [[567001.947009817, 6813423.48716989], [567003...                0  \n",
       "2       [[571569.725229183, 6814491.43055043], [571568...                0  \n",
       "3       [[511604.226279186, 6793164.49240385], [511606...                0  \n",
       "4       [[512670.53379004, 6788978.16589243], [512670....                0  \n",
       "...                                                   ...              ...  \n",
       "249995  [[567728.230127489, 6816773.19594685], [567729...  [0, 1, 2, 3, 4]  \n",
       "249996  [[567680.278299499, 6816931.16778507], [567681...  [0, 1, 2, 3, 4]  \n",
       "249997  [[567760.824596053, 6816701.11418839], [567759...  [0, 1, 2, 3, 4]  \n",
       "249998  [[592573.660072807, 6742824.04699339], [592566...  [0, 1, 2, 3, 4]  \n",
       "249999  [[517797.139952014, 6775080.54679077], [517818...  [0, 1, 2, 3, 4]  \n",
       "\n",
       "[250000 rows x 7 columns]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in results_df.iterrows():\n",
    "    if isinstance(row['algorithm_top5'], list) == False:\n",
    "        if isinstance(row['algorithm_top3'], list) == False:\n",
    "            results_df.at[index,'algorithm_top5'] = row['algorithm_top1']\n",
    "        else:\n",
    "            results_df.at[index,'algorithm_top5'] = row['algorithm_top3']\n",
    "            \n",
    "    if isinstance(row['algorithm_top3'], list) == False:\n",
    "        results_df.at[index,'algorithm_top3'] = row['algorithm_top1']\n",
    "    \n",
    "    results_df.at[index, 'algorithm_all'] = list(results_df['algorithm_all'][index])\n",
    "    \n",
    "results_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ddce86c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11    7056\n",
       "10    5000\n",
       "9     5000\n",
       "8     5000\n",
       "7     5000\n",
       "1     5000\n",
       "0     5000\n",
       "2     4226\n",
       "3     3292\n",
       "4     2352\n",
       "6     1859\n",
       "5     1537\n",
       "12     808\n",
       "Name: algorithm_top1, dtype: int64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_majority = results_df[results_df.algorithm_top1 == 0]\n",
    "df_minority = results_df[results_df.algorithm_top1 != 0]\n",
    "\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                  replace=False,\n",
    "                                  n_samples=5000,\n",
    "                                   #n_samples=500,\n",
    "                                  random_state=123)\n",
    "\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "df_majority = df_downsampled[df_downsampled.algorithm_top1 == 7]\n",
    "df_minority = df_downsampled[df_downsampled.algorithm_top1 != 7]\n",
    "\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                  replace=False,\n",
    "                                  n_samples=5000,\n",
    "                                   #n_samples=500,\n",
    "                                  random_state=123)\n",
    "\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "df_majority = df_downsampled[df_downsampled.algorithm_top1 == 8]\n",
    "df_minority = df_downsampled[df_downsampled.algorithm_top1 != 8]\n",
    "\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                  replace=False,\n",
    "                                  n_samples=5000,\n",
    "                                   #n_samples=500,\n",
    "                                  random_state=123)\n",
    "\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "df_majority = df_downsampled[df_downsampled.algorithm_top1 == 10]\n",
    "df_minority = df_downsampled[df_downsampled.algorithm_top1 != 10]\n",
    "\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                  replace=False,\n",
    "                                  n_samples=5000,\n",
    "                                   #n_samples=500,\n",
    "                                  random_state=123)\n",
    "\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "df_majority = df_downsampled[df_downsampled.algorithm_top1 == 9]\n",
    "df_minority = df_downsampled[df_downsampled.algorithm_top1 != 9]\n",
    "\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                  replace=False,\n",
    "                                  n_samples=5000,\n",
    "                                  # n_samples=500,\n",
    "                                  random_state=123)\n",
    "\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "df_majority = df_downsampled[df_downsampled.algorithm_top1 == 1]\n",
    "df_minority = df_downsampled[df_downsampled.algorithm_top1 != 1]\n",
    "\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                  replace=False,\n",
    "                                  n_samples=5000,\n",
    "                                  # n_samples=500,\n",
    "                                  random_state=123)\n",
    "\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "results_list = df_downsampled.to_dict('records')\n",
    "\n",
    "df_downsampled['algorithm_top1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "71535ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot(y,length):\n",
    "    output = []\n",
    "    for i in y:\n",
    "        array = np.zeros(length)\n",
    "        if isinstance(i, int) == False:\n",
    "            for j in i:\n",
    "                array[j] = 1\n",
    "        else:\n",
    "            array[i] = 1\n",
    "                \n",
    "        output.append(array)\n",
    "    return np.array(output)\n",
    "\n",
    "def make_one_one_hot(y,length):\n",
    "    array = np.zeros(length)\n",
    "    array[y] = 1\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ba0cdb",
   "metadata": {},
   "source": [
    "## Data processing top1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1874e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for element in results_list:\n",
    "    \n",
    "    X.append(element['polygon'])\n",
    "    y.append(element['algorithm_top1'])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "y = np.nan_to_num(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "y_onehottrain = to_categorical(y_train)\n",
    "train_tensor = []\n",
    "b=0\n",
    "for a in X_train:\n",
    "    d = y_onehottrain[b]\n",
    "    c = y_train[b]\n",
    "    train_tensor.append([a,c,d]) \n",
    "    b = b+1\n",
    "\n",
    "y_onehottest = to_categorical(y_test)\n",
    "test_tensor = []\n",
    "b=0\n",
    "for a in X_test:\n",
    "    d = y_onehottest[b]\n",
    "    c = y_test[b]\n",
    "    test_tensor.append([a,c,d])\n",
    "    b = b+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1be5a6",
   "metadata": {},
   "source": [
    "## Data processing top3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "83819756",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for element in results_list:\n",
    "    \n",
    "    X.append(element['polygon'])\n",
    "    y.append(element['algorithm_top3'])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "y = np.nan_to_num(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "y_onehottrain = make_one_hot(y_train, 13)\n",
    "train_tensor = []\n",
    "b=0\n",
    "for a in X_train:\n",
    "    d = y_onehottrain[b]\n",
    "    if isinstance(y_train[b], list) == True:\n",
    "        c = y_train[b][0]\n",
    "    else:\n",
    "        c = y_train[b]\n",
    "    train_tensor.append([a,c,d]) \n",
    "    b = b+1\n",
    "\n",
    "y_onehottest = make_one_hot(y_test, 13)\n",
    "test_tensor = []\n",
    "b=0\n",
    "for a in X_test:\n",
    "    d = y_onehottest[b]\n",
    "    if isinstance(y_test[b], list) == True:\n",
    "        c = y_test[b][0]\n",
    "    else:\n",
    "        c = y_test[b]\n",
    "    test_tensor.append([a,c,d])\n",
    "    b = b+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa686a0",
   "metadata": {},
   "source": [
    "## Data processing top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "592f90c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for element in results_list:\n",
    "    \n",
    "    X.append(element['polygon'])\n",
    "    y.append(element['algorithm_top5'])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "y = np.nan_to_num(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "y_onehottrain = make_one_hot(y_train, 13)\n",
    "train_tensor = []\n",
    "b=0\n",
    "for a in X_train:\n",
    "    d = y_onehottrain[b]\n",
    "    if isinstance(y_train[b], list) == True:\n",
    "        c = y_train[b][0]\n",
    "    else:\n",
    "        c = y_train[b]\n",
    "    train_tensor.append([a,c,d]) \n",
    "    b = b+1\n",
    "\n",
    "y_onehottest = make_one_hot(y_test, 13)\n",
    "test_tensor = []\n",
    "b=0\n",
    "for a in X_test:\n",
    "    d = y_onehottest[b]\n",
    "    if isinstance(y_test[b], list) == True:\n",
    "        c = y_test[b][0]\n",
    "    else:\n",
    "        c = y_test[b]\n",
    "    test_tensor.append([a,c,d])\n",
    "    b = b+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-pixel",
   "metadata": {},
   "source": [
    "## Data processing Approach 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "square-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_X = []\n",
    "\n",
    "for element in results_list:\n",
    "    pre_X.append(element['properties'])\n",
    "pre_X = np.array(pre_X)\n",
    "pre_X = np.nan_to_num(pre_X)   \n",
    "\n",
    "a=0\n",
    "for a in range(pre_X.shape[1]):\n",
    "    pre_X[:,a] = (pre_X[:,a] - pre_X[:,a].mean()) / pre_X[:,a].std()\n",
    "\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "a=0\n",
    "for element in results_list:\n",
    "    \n",
    "    X.append([element['polygon'], pre_X[a]])\n",
    "    y.append(element['algorithm_top5'])\n",
    "    a+=1\n",
    "X = np.array(X)\n",
    "X = np.nan_to_num(X)\n",
    "y = np.array(y)\n",
    "y = np.nan_to_num(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "y_onehottrain = make_one_hot(y_train, 13)\n",
    "train_tensor = []\n",
    "b=0\n",
    "for a in X_train:\n",
    "    d = y_onehottrain[b]\n",
    "    if isinstance(y_train[b], list) == True:\n",
    "        c = y_train[b][0]\n",
    "    else:\n",
    "        c = y_train[b]\n",
    "    train_tensor.append([a[0], a[1],c,d]) \n",
    "    b = b+1\n",
    "\n",
    "y_onehottest = make_one_hot(y_test, 13)\n",
    "test_tensor = []\n",
    "b=0\n",
    "for a in X_test:\n",
    "    d = y_onehottest[b]\n",
    "    if isinstance(y_test[b], list) == True:\n",
    "        c = y_test[b][0]\n",
    "    else:\n",
    "        c = y_test[b]\n",
    "    test_tensor.append([a[0],a[1],c,d])\n",
    "    b = b+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-fourth",
   "metadata": {},
   "source": [
    "## Check percentage of simplifications that exceeds visual variance condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "falling-punch",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_X = []\n",
    "\n",
    "for element in results_list:\n",
    "    pre_X.append(element['properties'])\n",
    "pre_X = np.array(pre_X)\n",
    "pre_X = np.nan_to_num(pre_X)   \n",
    "\n",
    "a=0\n",
    "for a in range(pre_X.shape[1]):\n",
    "    pre_X[:,a] = (pre_X[:,a] - pre_X[:,a].mean()) / pre_X[:,a].std()\n",
    "\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "a=0\n",
    "for element in results_list:\n",
    "    \n",
    "    X.append([element['polygon'], pre_X[a], element['original_geom']])\n",
    "    y.append(element['algorithm_all'])\n",
    "    a+=1\n",
    "X = np.array(X)\n",
    "X = np.nan_to_num(X)\n",
    "y = np.array(y)\n",
    "y = np.nan_to_num(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "y_onehottrain = make_one_hot(y_train, 13)\n",
    "train_tensor = []\n",
    "b=0\n",
    "for a in X_train:\n",
    "    d = y_onehottrain[b]\n",
    "    if isinstance(y_train[b], list) == True:\n",
    "        c = y_train[b][0]\n",
    "    else:\n",
    "        c = y_train[b]\n",
    "    train_tensor.append([a[0], a[1], a[2], c,d]) \n",
    "    b = b+1\n",
    "\n",
    "y_onehottest = make_one_hot(y_test, 13)\n",
    "test_tensor = []\n",
    "b=0\n",
    "for a in X_test:\n",
    "    d = y_onehottest[b]\n",
    "    if isinstance(y_test[b], list) == True:\n",
    "        c = y_test[b][0]\n",
    "    else:\n",
    "        c = y_test[b]\n",
    "    test_tensor.append([a[0],a[1], a[2],c,d])\n",
    "    b = b+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c8eb5",
   "metadata": {},
   "source": [
    "# Data Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0930fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'keys':list(Counter(y_test).keys()),\n",
    "              'freq':list(Counter(y_test).values())})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25c23f7",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "389a4005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([46693, 198])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MNIST_Polygon(Dataset):\n",
    "    \n",
    "    def __init__(self, tensor, transform=None):\n",
    "        data = [x for x, y, z in tensor]\n",
    "        data = np.asarray(data)\n",
    "        self.data = torch.reshape(torch.from_numpy(data).float(), (data.shape[0], data.shape[1]*data.shape[2]))\n",
    "        self.targets = [torch.tensor(y).long() for x, y, z in tensor]\n",
    "        self.onehot = [z for x, y, z in tensor]\n",
    "                \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "MNIST_Polygon(train_tensor).data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "48b5a288",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "LR = 0.002\n",
    "LOG_INTERVAL = 10\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(198, 256, bias=False),\n",
    "            #nn.BatchNorm1d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 128, bias=False), \n",
    "            #nn.BatchNorm1d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 13),\n",
    "            nn.Softmax(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.conv(x)\n",
    "        #x = torch.flatten(x, 1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "627887f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()), end=\"\\r\")\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, results_list, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    a = 0\n",
    "    real = dataset2.onehot\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            #correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            for i in pred:\n",
    "                correct += real[a][i]\n",
    "                a+=1\n",
    "            \n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    results_list.append([epoch, correct / len(test_loader.dataset), test_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e34bbd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [46080/46693 (99%)]\tLoss: 2.539632\n",
      "Test set: Average loss: 2.4779, Accuracy: 2387.0/5189 (46.0%)\n",
      "\n",
      "Train Epoch: 2 [46080/46693 (99%)]\tLoss: 2.446875\n",
      "Test set: Average loss: 2.4487, Accuracy: 2471.0/5189 (47.6%)\n",
      "\n",
      "Train Epoch: 3 [46080/46693 (99%)]\tLoss: 2.509770\n",
      "Test set: Average loss: 2.4452, Accuracy: 2492.0/5189 (48.0%)\n",
      "\n",
      "Train Epoch: 4 [46080/46693 (99%)]\tLoss: 2.424538\n",
      "Test set: Average loss: 2.4428, Accuracy: 2485.0/5189 (47.9%)\n",
      "\n",
      "Train Epoch: 5 [46080/46693 (99%)]\tLoss: 2.385988\n",
      "Test set: Average loss: 2.4386, Accuracy: 2477.0/5189 (47.7%)\n",
      "\n",
      "Train Epoch: 6 [46080/46693 (99%)]\tLoss: 2.461888\n",
      "Test set: Average loss: 2.4391, Accuracy: 2493.0/5189 (48.0%)\n",
      "\n",
      "Train Epoch: 7 [46080/46693 (99%)]\tLoss: 2.481779\n",
      "Test set: Average loss: 2.4336, Accuracy: 2502.0/5189 (48.2%)\n",
      "\n",
      "Train Epoch: 8 [46080/46693 (99%)]\tLoss: 2.442126\n",
      "Test set: Average loss: 2.4430, Accuracy: 2428.0/5189 (46.8%)\n",
      "\n",
      "Train Epoch: 9 [46080/46693 (99%)]\tLoss: 2.371488\n",
      "Test set: Average loss: 2.4321, Accuracy: 2489.0/5189 (48.0%)\n",
      "\n",
      "Train Epoch: 10 [46080/46693 (99%)]\tLoss: 2.449002\n",
      "Test set: Average loss: 2.4374, Accuracy: 2515.0/5189 (48.5%)\n",
      "\n",
      "Train Epoch: 11 [46080/46693 (99%)]\tLoss: 2.364079\n",
      "Test set: Average loss: 2.4310, Accuracy: 2514.0/5189 (48.4%)\n",
      "\n",
      "Train Epoch: 12 [46080/46693 (99%)]\tLoss: 2.401691\n",
      "Test set: Average loss: 2.4302, Accuracy: 2555.0/5189 (49.2%)\n",
      "\n",
      "Train Epoch: 13 [46080/46693 (99%)]\tLoss: 2.483712\n",
      "Test set: Average loss: 2.4313, Accuracy: 2556.0/5189 (49.3%)\n",
      "\n",
      "Train Epoch: 14 [46080/46693 (99%)]\tLoss: 2.466298\n",
      "Test set: Average loss: 2.4290, Accuracy: 2556.0/5189 (49.3%)\n",
      "\n",
      "Train Epoch: 15 [46080/46693 (99%)]\tLoss: 2.401897\n",
      "Test set: Average loss: 2.4354, Accuracy: 2559.0/5189 (49.3%)\n",
      "\n",
      "Train Epoch: 16 [46080/46693 (99%)]\tLoss: 2.520127\n",
      "Test set: Average loss: 2.4287, Accuracy: 2536.0/5189 (48.9%)\n",
      "\n",
      "Train Epoch: 17 [46080/46693 (99%)]\tLoss: 2.423847\n",
      "Test set: Average loss: 2.4295, Accuracy: 2560.0/5189 (49.3%)\n",
      "\n",
      "Train Epoch: 18 [46080/46693 (99%)]\tLoss: 2.530503\n",
      "Test set: Average loss: 2.4310, Accuracy: 2539.0/5189 (48.9%)\n",
      "\n",
      "Train Epoch: 19 [46080/46693 (99%)]\tLoss: 2.347059\n",
      "Test set: Average loss: 2.4287, Accuracy: 2565.0/5189 (49.4%)\n",
      "\n",
      "Train Epoch: 20 [46080/46693 (99%)]\tLoss: 2.490321\n",
      "Test set: Average loss: 2.4286, Accuracy: 2565.0/5189 (49.4%)\n",
      "\n",
      "Train Epoch: 21 [46080/46693 (99%)]\tLoss: 2.463107\n",
      "Test set: Average loss: 2.4276, Accuracy: 2584.0/5189 (49.8%)\n",
      "\n",
      "Train Epoch: 22 [46080/46693 (99%)]\tLoss: 2.414746\n",
      "Test set: Average loss: 2.4264, Accuracy: 2600.0/5189 (50.1%)\n",
      "\n",
      "Train Epoch: 23 [46080/46693 (99%)]\tLoss: 2.471988\n",
      "Test set: Average loss: 2.4284, Accuracy: 2561.0/5189 (49.4%)\n",
      "\n",
      "Train Epoch: 24 [46080/46693 (99%)]\tLoss: 2.309464\n",
      "Test set: Average loss: 2.4275, Accuracy: 2567.0/5189 (49.5%)\n",
      "\n",
      "Train Epoch: 25 [46080/46693 (99%)]\tLoss: 2.400545\n",
      "Test set: Average loss: 2.4276, Accuracy: 2540.0/5189 (48.9%)\n",
      "\n",
      "Train Epoch: 26 [46080/46693 (99%)]\tLoss: 2.428409\n",
      "Test set: Average loss: 2.4291, Accuracy: 2565.0/5189 (49.4%)\n",
      "\n",
      "Train Epoch: 27 [46080/46693 (99%)]\tLoss: 2.395285\n",
      "Test set: Average loss: 2.4305, Accuracy: 2553.0/5189 (49.2%)\n",
      "\n",
      "Train Epoch: 28 [46080/46693 (99%)]\tLoss: 2.359268\n",
      "Test set: Average loss: 2.4262, Accuracy: 2576.0/5189 (49.6%)\n",
      "\n",
      "Train Epoch: 29 [46080/46693 (99%)]\tLoss: 2.397679\n",
      "Test set: Average loss: 2.4294, Accuracy: 2612.0/5189 (50.3%)\n",
      "\n",
      "Train Epoch: 30 [46080/46693 (99%)]\tLoss: 2.433817\n",
      "Test set: Average loss: 2.4266, Accuracy: 2590.0/5189 (49.9%)\n",
      "\n",
      "Train Epoch: 31 [46080/46693 (99%)]\tLoss: 2.483644\n",
      "Test set: Average loss: 2.4349, Accuracy: 2538.0/5189 (48.9%)\n",
      "\n",
      "Train Epoch: 32 [46080/46693 (99%)]\tLoss: 2.518707\n",
      "Test set: Average loss: 2.4290, Accuracy: 2534.0/5189 (48.8%)\n",
      "\n",
      "Train Epoch: 33 [46080/46693 (99%)]\tLoss: 2.442751\n",
      "Test set: Average loss: 2.4299, Accuracy: 2542.0/5189 (49.0%)\n",
      "\n",
      "Train Epoch: 34 [46080/46693 (99%)]\tLoss: 2.481088\n",
      "Test set: Average loss: 2.4336, Accuracy: 2579.0/5189 (49.7%)\n",
      "\n",
      "Train Epoch: 35 [46080/46693 (99%)]\tLoss: 2.430209\n",
      "Test set: Average loss: 2.4253, Accuracy: 2618.0/5189 (50.5%)\n",
      "\n",
      "Train Epoch: 36 [46080/46693 (99%)]\tLoss: 2.468679\n",
      "Test set: Average loss: 2.4257, Accuracy: 2614.0/5189 (50.4%)\n",
      "\n",
      "Train Epoch: 37 [46080/46693 (99%)]\tLoss: 2.352140\n",
      "Test set: Average loss: 2.4291, Accuracy: 2562.0/5189 (49.4%)\n",
      "\n",
      "Train Epoch: 38 [46080/46693 (99%)]\tLoss: 2.348972\n",
      "Test set: Average loss: 2.4255, Accuracy: 2622.0/5189 (50.5%)\n",
      "\n",
      "Train Epoch: 39 [46080/46693 (99%)]\tLoss: 2.385140\n",
      "Test set: Average loss: 2.4255, Accuracy: 2594.0/5189 (50.0%)\n",
      "\n",
      "Train Epoch: 40 [46080/46693 (99%)]\tLoss: 2.363203\n",
      "Test set: Average loss: 2.4257, Accuracy: 2602.0/5189 (50.1%)\n",
      "\n",
      "Train Epoch: 41 [46080/46693 (99%)]\tLoss: 2.403886\n",
      "Test set: Average loss: 2.4257, Accuracy: 2608.0/5189 (50.3%)\n",
      "\n",
      "Train Epoch: 42 [46080/46693 (99%)]\tLoss: 2.369831\n",
      "Test set: Average loss: 2.4227, Accuracy: 2614.0/5189 (50.4%)\n",
      "\n",
      "Train Epoch: 43 [46080/46693 (99%)]\tLoss: 2.418127\n",
      "Test set: Average loss: 2.4251, Accuracy: 2613.0/5189 (50.4%)\n",
      "\n",
      "Train Epoch: 44 [46080/46693 (99%)]\tLoss: 2.407333\n",
      "Test set: Average loss: 2.4261, Accuracy: 2616.0/5189 (50.4%)\n",
      "\n",
      "Train Epoch: 45 [46080/46693 (99%)]\tLoss: 2.431143\n",
      "Test set: Average loss: 2.4308, Accuracy: 2585.0/5189 (49.8%)\n",
      "\n",
      "Train Epoch: 46 [46080/46693 (99%)]\tLoss: 2.296250\n",
      "Test set: Average loss: 2.4231, Accuracy: 2637.0/5189 (50.8%)\n",
      "\n",
      "Train Epoch: 47 [46080/46693 (99%)]\tLoss: 2.346075\n",
      "Test set: Average loss: 2.4228, Accuracy: 2613.0/5189 (50.4%)\n",
      "\n",
      "Train Epoch: 48 [46080/46693 (99%)]\tLoss: 2.364214\n",
      "Test set: Average loss: 2.4222, Accuracy: 2616.0/5189 (50.4%)\n",
      "\n",
      "Train Epoch: 49 [46080/46693 (99%)]\tLoss: 2.448851\n",
      "Test set: Average loss: 2.4227, Accuracy: 2626.0/5189 (50.6%)\n",
      "\n",
      "Train Epoch: 50 [46080/46693 (99%)]\tLoss: 2.440450\n",
      "Test set: Average loss: 2.4283, Accuracy: 2598.0/5189 (50.1%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset1 = MNIST_Polygon(train_tensor)\n",
    "dataset2 = MNIST_Polygon(test_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,batch_size=BATCH_SIZE, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "model = Net().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "results_list = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    test(model, DEVICE, test_loader, results_list, epoch)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831abc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bcb6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_list)\n",
    "results_df.columns = ['epoch', 'accuracy', 'loss']\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b171802a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51882, 10)\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "a = 0\n",
    "for element in results_list:\n",
    "    \n",
    "    X.append(element['properties'])\n",
    "    y.append(element['algorithm_top5'])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X = np.nan_to_num(X)\n",
    "y = np.nan_to_num(y)\n",
    "\n",
    "a=0\n",
    "#for a in range(X.shape[1]):\n",
    "#    X[:,a] = (X[:,a] - X[:,a].mean()) / X[:,a].std()\n",
    "print(X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "y_onehottrain = make_one_hot(y_train, 13)\n",
    "#y_onehottrain = to_categorical(y_train)\n",
    "train_tensor = []\n",
    "b=0\n",
    "for a in X_train:\n",
    "    d = y_onehottrain[b]\n",
    "    if isinstance(y_train[b], list) == True:\n",
    "        c = y_train[b][0]\n",
    "    else:\n",
    "        c = y_train[b]\n",
    "    train_tensor.append([a,c,d]) \n",
    "    b = b+1\n",
    "\n",
    "y_onehottest = make_one_hot(y_test, 13)\n",
    "#y_onehottest = to_categorical(y_test)\n",
    "test_tensor = []\n",
    "b=0\n",
    "for a in X_test:\n",
    "    d = y_onehottest[b]\n",
    "    if isinstance(y_test[b], list) == True:\n",
    "        c = y_test[b][0]\n",
    "    else:\n",
    "        c = y_test[b]\n",
    "    test_tensor.append([a,c,d])\n",
    "    b = b+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "79730c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([46693, 10])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MNIST_Polygon2(Dataset):\n",
    "    \n",
    "    def __init__(self, tensor, transform=None):\n",
    "        data = [x for x, y, z in tensor]\n",
    "        self.data = torch.Tensor(data).float()\n",
    "        #self.data = torch.reshape(torch.from_numpy(data).float(), (data.shape[0], data.shape[1]*data.shape[2]))\n",
    "        self.targets = [torch.tensor(y).long() for x, y, z in tensor]\n",
    "        self.onehot = [z for x, y, z in tensor]\n",
    "                \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "MNIST_Polygon2(train_tensor).data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b8512270",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "LR = 0.002\n",
    "LOG_INTERVAL = 10\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(10, 64, bias=False),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 32, bias=False), \n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 13),\n",
    "            nn.Softmax(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.conv(x)\n",
    "        #x = torch.flatten(x, 1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4c9c57a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()), end=\"\\r\")\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, results_list, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    a = 0\n",
    "    real = dataset2.onehot\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            #correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            for i in pred:\n",
    "                correct += real[a][i]\n",
    "                a+=1\n",
    "            \n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    results_list.append([epoch, correct / len(test_loader.dataset), test_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "64d1f158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [46080/46693 (99%)]\tLoss: 2.519267\n",
      "Test set: Average loss: 2.4243, Accuracy: 2683.0/5189 (51.7%)\n",
      "\n",
      "Train Epoch: 2 [46080/46693 (99%)]\tLoss: 2.459669\n",
      "Test set: Average loss: 2.4184, Accuracy: 2660.0/5189 (51.3%)\n",
      "\n",
      "Train Epoch: 3 [46080/46693 (99%)]\tLoss: 2.447961\n",
      "Test set: Average loss: 2.4300, Accuracy: 2765.0/5189 (53.3%)\n",
      "\n",
      "Train Epoch: 4 [46080/46693 (99%)]\tLoss: 2.449836\n",
      "Test set: Average loss: 2.4115, Accuracy: 2719.0/5189 (52.4%)\n",
      "\n",
      "Train Epoch: 5 [46080/46693 (99%)]\tLoss: 2.373807\n",
      "Test set: Average loss: 2.4149, Accuracy: 2757.0/5189 (53.1%)\n",
      "\n",
      "Train Epoch: 6 [46080/46693 (99%)]\tLoss: 2.377857\n",
      "Test set: Average loss: 2.4082, Accuracy: 2700.0/5189 (52.0%)\n",
      "\n",
      "Train Epoch: 7 [46080/46693 (99%)]\tLoss: 2.446448\n",
      "Test set: Average loss: 2.4063, Accuracy: 2778.0/5189 (53.5%)\n",
      "\n",
      "Train Epoch: 8 [46080/46693 (99%)]\tLoss: 2.396425\n",
      "Test set: Average loss: 2.4071, Accuracy: 2813.0/5189 (54.2%)\n",
      "\n",
      "Train Epoch: 9 [46080/46693 (99%)]\tLoss: 2.437042\n",
      "Test set: Average loss: 2.4106, Accuracy: 2705.0/5189 (52.1%)\n",
      "\n",
      "Train Epoch: 10 [46080/46693 (99%)]\tLoss: 2.478990\n",
      "Test set: Average loss: 2.4074, Accuracy: 2763.0/5189 (53.2%)\n",
      "\n",
      "Train Epoch: 11 [46080/46693 (99%)]\tLoss: 2.391007\n",
      "Test set: Average loss: 2.4019, Accuracy: 2773.0/5189 (53.4%)\n",
      "\n",
      "Train Epoch: 12 [46080/46693 (99%)]\tLoss: 2.390943\n",
      "Test set: Average loss: 2.4115, Accuracy: 2811.0/5189 (54.2%)\n",
      "\n",
      "Train Epoch: 13 [46080/46693 (99%)]\tLoss: 2.484933\n",
      "Test set: Average loss: 2.4032, Accuracy: 2808.0/5189 (54.1%)\n",
      "\n",
      "Train Epoch: 14 [46080/46693 (99%)]\tLoss: 2.418551\n",
      "Test set: Average loss: 2.4040, Accuracy: 2825.0/5189 (54.4%)\n",
      "\n",
      "Train Epoch: 15 [46080/46693 (99%)]\tLoss: 2.422622\n",
      "Test set: Average loss: 2.4096, Accuracy: 2862.0/5189 (55.2%)\n",
      "\n",
      "Train Epoch: 16 [46080/46693 (99%)]\tLoss: 2.339474\n",
      "Test set: Average loss: 2.4012, Accuracy: 2786.0/5189 (53.7%)\n",
      "\n",
      "Train Epoch: 17 [46080/46693 (99%)]\tLoss: 2.423852\n",
      "Test set: Average loss: 2.3983, Accuracy: 2850.0/5189 (54.9%)\n",
      "\n",
      "Train Epoch: 18 [46080/46693 (99%)]\tLoss: 2.416557\n",
      "Test set: Average loss: 2.4024, Accuracy: 2902.0/5189 (55.9%)\n",
      "\n",
      "Train Epoch: 19 [46080/46693 (99%)]\tLoss: 2.427639\n",
      "Test set: Average loss: 2.4041, Accuracy: 2851.0/5189 (54.9%)\n",
      "\n",
      "Train Epoch: 20 [46080/46693 (99%)]\tLoss: 2.454188\n",
      "Test set: Average loss: 2.4007, Accuracy: 2865.0/5189 (55.2%)\n",
      "\n",
      "Train Epoch: 21 [46080/46693 (99%)]\tLoss: 2.372823\n",
      "Test set: Average loss: 2.4089, Accuracy: 2932.0/5189 (56.5%)\n",
      "\n",
      "Train Epoch: 22 [46080/46693 (99%)]\tLoss: 2.369800\n",
      "Test set: Average loss: 2.3960, Accuracy: 2879.0/5189 (55.5%)\n",
      "\n",
      "Train Epoch: 23 [46080/46693 (99%)]\tLoss: 2.406926\n",
      "Test set: Average loss: 2.3960, Accuracy: 2907.0/5189 (56.0%)\n",
      "\n",
      "Train Epoch: 24 [46080/46693 (99%)]\tLoss: 2.354132\n",
      "Test set: Average loss: 2.3952, Accuracy: 2830.0/5189 (54.5%)\n",
      "\n",
      "Train Epoch: 25 [46080/46693 (99%)]\tLoss: 2.398191\n",
      "Test set: Average loss: 2.4047, Accuracy: 2898.0/5189 (55.8%)\n",
      "\n",
      "Train Epoch: 26 [46080/46693 (99%)]\tLoss: 2.353446\n",
      "Test set: Average loss: 2.4013, Accuracy: 2882.0/5189 (55.5%)\n",
      "\n",
      "Train Epoch: 27 [46080/46693 (99%)]\tLoss: 2.377539\n",
      "Test set: Average loss: 2.4003, Accuracy: 2899.0/5189 (55.9%)\n",
      "\n",
      "Train Epoch: 28 [46080/46693 (99%)]\tLoss: 2.479687\n",
      "Test set: Average loss: 2.3981, Accuracy: 2861.0/5189 (55.1%)\n",
      "\n",
      "Train Epoch: 29 [46080/46693 (99%)]\tLoss: 2.433905\n",
      "Test set: Average loss: 2.3985, Accuracy: 2938.0/5189 (56.6%)\n",
      "\n",
      "Train Epoch: 30 [46080/46693 (99%)]\tLoss: 2.424315\n",
      "Test set: Average loss: 2.4002, Accuracy: 2913.0/5189 (56.1%)\n",
      "\n",
      "Train Epoch: 31 [46080/46693 (99%)]\tLoss: 2.401846\n",
      "Test set: Average loss: 2.3946, Accuracy: 2906.0/5189 (56.0%)\n",
      "\n",
      "Train Epoch: 32 [46080/46693 (99%)]\tLoss: 2.264298\n",
      "Test set: Average loss: 2.3943, Accuracy: 2894.0/5189 (55.8%)\n",
      "\n",
      "Train Epoch: 33 [46080/46693 (99%)]\tLoss: 2.439790\n",
      "Test set: Average loss: 2.3920, Accuracy: 2883.0/5189 (55.6%)\n",
      "\n",
      "Train Epoch: 34 [46080/46693 (99%)]\tLoss: 2.395105\n",
      "Test set: Average loss: 2.3983, Accuracy: 2869.0/5189 (55.3%)\n",
      "\n",
      "Train Epoch: 35 [46080/46693 (99%)]\tLoss: 2.412791\n",
      "Test set: Average loss: 2.3897, Accuracy: 2893.0/5189 (55.8%)\n",
      "\n",
      "Train Epoch: 36 [46080/46693 (99%)]\tLoss: 2.433682\n",
      "Test set: Average loss: 2.3912, Accuracy: 2952.0/5189 (56.9%)\n",
      "\n",
      "Train Epoch: 37 [46080/46693 (99%)]\tLoss: 2.416843\n",
      "Test set: Average loss: 2.4104, Accuracy: 2876.0/5189 (55.4%)\n",
      "\n",
      "Train Epoch: 38 [46080/46693 (99%)]\tLoss: 2.428893\n",
      "Test set: Average loss: 2.4021, Accuracy: 2884.0/5189 (55.6%)\n",
      "\n",
      "Train Epoch: 39 [46080/46693 (99%)]\tLoss: 2.424316\n",
      "Test set: Average loss: 2.4001, Accuracy: 2955.0/5189 (56.9%)\n",
      "\n",
      "Train Epoch: 40 [46080/46693 (99%)]\tLoss: 2.294509\n",
      "Test set: Average loss: 2.3935, Accuracy: 2904.0/5189 (56.0%)\n",
      "\n",
      "Train Epoch: 41 [46080/46693 (99%)]\tLoss: 2.384413\n",
      "Test set: Average loss: 2.3989, Accuracy: 2904.0/5189 (56.0%)\n",
      "\n",
      "Train Epoch: 42 [46080/46693 (99%)]\tLoss: 2.321631\n",
      "Test set: Average loss: 2.3962, Accuracy: 2814.0/5189 (54.2%)\n",
      "\n",
      "Train Epoch: 43 [46080/46693 (99%)]\tLoss: 2.395379\n",
      "Test set: Average loss: 2.3998, Accuracy: 2908.0/5189 (56.0%)\n",
      "\n",
      "Train Epoch: 44 [46080/46693 (99%)]\tLoss: 2.368753\n",
      "Test set: Average loss: 2.3919, Accuracy: 2879.0/5189 (55.5%)\n",
      "\n",
      "Train Epoch: 45 [46080/46693 (99%)]\tLoss: 2.366242\n",
      "Test set: Average loss: 2.4003, Accuracy: 2868.0/5189 (55.3%)\n",
      "\n",
      "Train Epoch: 46 [46080/46693 (99%)]\tLoss: 2.346808\n",
      "Test set: Average loss: 2.3969, Accuracy: 2923.0/5189 (56.3%)\n",
      "\n",
      "Train Epoch: 47 [46080/46693 (99%)]\tLoss: 2.448272\n",
      "Test set: Average loss: 2.4034, Accuracy: 2950.0/5189 (56.9%)\n",
      "\n",
      "Train Epoch: 48 [46080/46693 (99%)]\tLoss: 2.492754\n",
      "Test set: Average loss: 2.3951, Accuracy: 2819.0/5189 (54.3%)\n",
      "\n",
      "Train Epoch: 49 [46080/46693 (99%)]\tLoss: 2.421876\n",
      "Test set: Average loss: 2.4041, Accuracy: 2981.0/5189 (57.4%)\n",
      "\n",
      "Train Epoch: 50 [46080/46693 (99%)]\tLoss: 2.403890\n",
      "Test set: Average loss: 2.3953, Accuracy: 2857.0/5189 (55.1%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset1 = MNIST_Polygon2(train_tensor)\n",
    "dataset2 = MNIST_Polygon2(test_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,batch_size=BATCH_SIZE, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "model = Net().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "results_list = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    test(model, DEVICE, test_loader, results_list, epoch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2967b247",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 64\n",
    "EPOCHS = 14\n",
    "LR = 0.002\n",
    "LOG_INTERVAL = 10\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(10, 64, bias=False),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 32, bias=False), \n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, len(y_onehot[0])),\n",
    "            nn.Softmax(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.conv(x)\n",
    "        #x = torch.flatten(x, 1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde75cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()), end=\"\\r\")\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, results_list, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    results_list.append([epoch, correct / len(test_loader.dataset), test_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c1c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = MNIST_Polygon2(train_tensor)\n",
    "dataset2 = MNIST_Polygon2(test_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,batch_size=BATCH_SIZE, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=TEST_BATCH_SIZE, shuffle = True)\n",
    "\n",
    "model = Net().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "results_list = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    test(model, DEVICE, test_loader, results_list, epoch)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-fusion",
   "metadata": {},
   "source": [
    "## Approach 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "adolescent-sixth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([46017, 198])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MNIST_Polygon3(Dataset):\n",
    "    \n",
    "    def __init__(self, tensor, transform=None):\n",
    "        data1 = [x1 for x1, x2, x3, y, z in tensor]\n",
    "        data1 = np.asarray(data1)\n",
    "        self.data1 = torch.reshape(torch.from_numpy(data1).float(), (data1.shape[0], data1.shape[1]*data1.shape[2]))\n",
    "        data2 = [x2 for x1, x2, x3, y, z in tensor]\n",
    "        self.data2 = torch.Tensor(data2).float()\n",
    "        self.data3 = np.asarray([x3 for x1, x2, x3, y, z in tensor])\n",
    "        self.targets = torch.Tensor([torch.tensor(y).long() for x1, x2, x3, y, z in tensor])\n",
    "        self.onehot = [z for x1, x2, x3, y, z in tensor]\n",
    "                \n",
    "    def __getitem__(self, index):\n",
    "        x1 = self.data1[index]\n",
    "        x2 = self.data2[index]\n",
    "        y = self.targets[index]\n",
    "        return x1, x2, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data1)\n",
    "    \n",
    "MNIST_Polygon3(train_tensor).data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "02e1cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 5113\n",
    "EPOCHS = 10\n",
    "LR = 0.002\n",
    "LOG_INTERVAL = 10\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class SymmetryPlusData(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.poly_net = nn.Sequential(\n",
    "            nn.Linear(198, 256, bias=False),\n",
    "            #nn.BatchNorm1d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 128, bias=False), \n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(10, 64, bias=False),\n",
    "            #nn.BatchNorm1d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 32, bias=False), \n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            #nn.BatchNorm1d(160),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(160, 64),\n",
    "            #nn.BatchNorm1d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 13),\n",
    "            nn.Softmax(1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, poly, features):\n",
    "        #print(\"poly: \" + str(poly))\n",
    "        #print(\"features: \" + str(features))\n",
    "        y1 = self.poly_net(poly)\n",
    "        y2 = self.feature_net(features)\n",
    "        y = torch.cat([y1, y2], -1)\n",
    "        return self.classifier(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "provincial-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for batch_idx, (data1, target , data2) in enumerate(train_loader):\n",
    "        data1, data2, target = data1.to(device), data2.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data1, data2)\n",
    "        loss = criterion(output, target.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data1), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()), end=\"\\r\")\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, results_list, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    a = 0\n",
    "    real = datasetb.onehot\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    with torch.no_grad():\n",
    "        for data1, target, data2 in test_loader:\n",
    "            data1, target, data2 = data1.to(device), target.to(device), data2.to(device)\n",
    "            output = model(data1, data2)\n",
    "            test_loss += criterion(output, target.long()).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            #correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            for i in pred:\n",
    "                correct += real[a][i]\n",
    "                a+=1\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    results_list.append([epoch, correct / len(test_loader.dataset), test_loss])\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "pending-projection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [45440/46017 (99%)]\tLoss: 2.460063\n",
      "Test set: Average loss: 2.4428, Accuracy: 3571.0/5113 (69.8%)\n",
      "\n",
      "Train Epoch: 2 [45440/46017 (99%)]\tLoss: 2.376543\n",
      "Test set: Average loss: 2.4310, Accuracy: 3720.0/5113 (72.8%)\n",
      "\n",
      "Train Epoch: 3 [45440/46017 (99%)]\tLoss: 2.489886\n",
      "Test set: Average loss: 2.4289, Accuracy: 4049.0/5113 (79.2%)\n",
      "\n",
      "Train Epoch: 4 [45440/46017 (99%)]\tLoss: 2.453550\n",
      "Test set: Average loss: 2.4237, Accuracy: 3716.0/5113 (72.7%)\n",
      "\n",
      "Train Epoch: 5 [45440/46017 (99%)]\tLoss: 2.474199\n",
      "Test set: Average loss: 2.4222, Accuracy: 3790.0/5113 (74.1%)\n",
      "\n",
      "Train Epoch: 6 [45440/46017 (99%)]\tLoss: 2.450637\n",
      "Test set: Average loss: 2.4218, Accuracy: 3904.0/5113 (76.4%)\n",
      "\n",
      "Train Epoch: 7 [45440/46017 (99%)]\tLoss: 2.451126\n",
      "Test set: Average loss: 2.4231, Accuracy: 3859.0/5113 (75.5%)\n",
      "\n",
      "Train Epoch: 8 [45440/46017 (99%)]\tLoss: 2.351013\n",
      "Test set: Average loss: 2.4188, Accuracy: 3893.0/5113 (76.1%)\n",
      "\n",
      "Train Epoch: 9 [45440/46017 (99%)]\tLoss: 2.397688\n",
      "Test set: Average loss: 2.4180, Accuracy: 3868.0/5113 (75.7%)\n",
      "\n",
      "Train Epoch: 10 [45440/46017 (99%)]\tLoss: 2.397256\n",
      "Test set: Average loss: 2.4205, Accuracy: 3856.0/5113 (75.4%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataseta = MNIST_Polygon3(train_tensor)\n",
    "dataset1 = torch.utils.data.TensorDataset(dataseta.data1, dataseta.targets, dataseta.data2)\n",
    "datasetb = MNIST_Polygon3(test_tensor)\n",
    "dataset2 = torch.utils.data.TensorDataset(datasetb.data1, datasetb.targets, datasetb.data2)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,batch_size=BATCH_SIZE, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "model = SymmetryPlusData().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "results_list = []\n",
    "preds = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    test(model, DEVICE, test_loader, results_list, epoch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "neither-valve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107243\n",
      "91874\n"
     ]
    }
   ],
   "source": [
    "original_point_counter = 0\n",
    "new_point_counter = 0\n",
    "new_coords = []\n",
    "a = 0\n",
    "for geom in datasetb.data1:\n",
    "    geom = datasetb.data3[a]\n",
    "    pred = preds[2][a]\n",
    "    original_point_counter += len(geom)\n",
    "    if pred == 0:\n",
    "        new_point_counter += len(geom)\n",
    "    elif (pred > 0) & (pred < 12):\n",
    "        simp = simplify_possibilities[pred]\n",
    "        if simp[0] == 'D-P':\n",
    "            # Simplification function Douglas-Peucker\n",
    "            simplified_coordinates = simplify_coords(geom, simp[1])\n",
    "            new_point_counter += len(simplified_coordinates)\n",
    "            new_coords.append(simplified_coordinates)\n",
    "\n",
    "        if simp[0] == 'V-W':\n",
    "            # Simplification function Visvalingam-Whyatt\n",
    "            simplified_coordinates = simplify_coords_vw(geom, simp[1])\n",
    "            new_point_counter += len(simplified_coordinates)\n",
    "            new_coords.append(simplified_coordinates)\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    a+=1\n",
    "    \n",
    "print(original_point_counter)\n",
    "print(new_point_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "duplicate-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(new_coords)\n",
    "\n",
    "pickle.dump(datasetb.data3, open( \"/Users/davemeijdam/Documents/Data Science/Master/Master Thesis/Scripts/data/temp/original_coords.p\", \"wb\" ) )\n",
    "pickle.dump(np.asarray(new_coords), open( \"/Users/davemeijdam/Documents/Data Science/Master/Master Thesis/Scripts/data/temp/simplified_coords.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "hybrid-noise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8321573980372314\n",
      "0.8566899471294164\n"
     ]
    }
   ],
   "source": [
    "o = 2761.915\n",
    "n = 2298.348\n",
    "\n",
    "print(n / o)\n",
    "\n",
    "o = 107243\n",
    "n = 91874\n",
    "\n",
    "print(n / o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517e7f17",
   "metadata": {},
   "source": [
    "# Experiments & Code that doesn't get used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3805662c",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bec35ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X[0].shape\n",
    "print(input_shape)\n",
    "print(len(y_onehot[0]))\n",
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Conv1D(filters=32, kernel_size=(5,), activation='relu', padding='SAME', input_shape=input_shape))\n",
    "model.add(layers.MaxPooling1D(3))\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=(5,), activation='relu', padding='SAME', strides=2))\n",
    "model.add(layers.GlobalAveragePooling1D())\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(len(y_onehot[0]), activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2285005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE = 99\n",
    "EPOCHS = 3\n",
    "\n",
    "history = model.fit(X,\n",
    "                    y_onehot,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7154e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_pand_centrum = create_connection(\"/Users/davemeijdam/Documents/Data Science/Master/Master Thesis/Data/SQLite/Pand_26116_centrum.db\")\n",
    "\n",
    "cur = conn_pand_centrum.cursor()\n",
    "cur.execute(\"SELECT data FROM tiles;\")\n",
    "\n",
    "rows = cur.fetchall()\n",
    "pand_centrum_data = []\n",
    "for row in rows:\n",
    "    pand_centrum_data.append(mapbox_vector_tile.decode(row[0]))\n",
    "    #print(row[0])\n",
    "print(len(pand_centrum_data))\n",
    "\n",
    "## Wegdeel Buiten\n",
    "\n",
    "conn_wegdeel_buiten = create_connection(\"/Users/davemeijdam/Documents/Data Science/Master/Master Thesis/Data/SQLite/Wegdeel_23770_buitengebied.db\")\n",
    "\n",
    "cur = conn_wegdeel_buiten.cursor()\n",
    "cur.execute(\"SELECT data FROM tiles;\")\n",
    "\n",
    "rows = cur.fetchall()\n",
    "wegdeel_buiten_data = []\n",
    "for row in rows:\n",
    "    wegdeel_buiten_data.append(mapbox_vector_tile.decode(row[0]))\n",
    "\n",
    "\n",
    "Lines = []\n",
    "Polygons = []\n",
    "MultiPolygons = []\n",
    "a=0\n",
    "for row in pand_centrum_data[:10000]:\n",
    "    print(str(a) + \" / \" + str(len(pand_centrum_data)), end=\"\\r\")\n",
    "    a = a + 1\n",
    "    keys = row.keys()\n",
    "    \n",
    "    for key in keys:\n",
    "        for element in row[key]['features']:\n",
    "            \n",
    "            if element['geometry']['type'] == 'LineString': \n",
    "                Lines.append(element['geometry']['coordinates'])\n",
    "            \n",
    "            if element['geometry']['type'] == 'Polygon':\n",
    "                Polygons.append(element['geometry']['coordinates'][0])\n",
    "                \n",
    "            #if element['geometry']['type'] == 'MultiPolygon':\n",
    "                #MultiPolygons.append(element['geometry']['coordinates'])\n",
    "    \n",
    "    \n",
    "\n",
    "#test = lvl10_data[0]['spoor.se_fld12_lijngeometrie2d']['features'][0]['geometry']['coordinates']\n",
    "#print(Polygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9e6b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely.geometry as sg\n",
    "import shapely.ops as so\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ls = []\n",
    "#for a in wegdeeljson['features'][:5]:\n",
    "#    ls.append(geometry.Polygon(a['geometry']['coordinates'][0]))\n",
    "\n",
    "new_shape = so.cascaded_union(ls)\n",
    "fig, axs = plt.subplots()\n",
    "axs.set_aspect('equal', 'datalim')\n",
    "\n",
    "for geom in new_shape.geoms:    \n",
    "    xs, ys = geom.exterior.xy    \n",
    "    axs.fill(xs, ys, alpha=1, fc='r', ec='none')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46104a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely.geometry as sg\n",
    "import shapely.ops as so\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "ls = []\n",
    "for element in wegdeel_buiten_data[3]['wegdeel.se_fld15_vlakgeometrie2d']['features']:\n",
    "    \n",
    "    #print(element['geometry']['coordinates'][0])\n",
    "    #geometry.Polygon(element['geometry']['coordinates'][0])\n",
    "    element2 = element['geometry']\n",
    "    \n",
    "    if element2['type'] == 'MultiPolygon':\n",
    "        if element2['coordinates']:\n",
    "            for poly in element2['coordinates'][0]:\n",
    "                print(poly)\n",
    "                ls.append(geometry.Polygon(poly))\n",
    "    \n",
    "    else:\n",
    "        ls.append(geometry.Polygon(element['geometry']['coordinates'][0]))\n",
    "\n",
    "#r1 = sg.Polygon([[243, 2760], [242, 2760], [242, 2761], [243, 2760]])\n",
    "#r2 = sg.Polygon([[243, 2759], [243, 2760], [244, 2760], [244, 2759], [243, 2759]])\n",
    "#r3 = sg.Polygon([[244, 2759], [243, 2759], [243, 2760], [244, 2760], [244, 2759]])\n",
    "#r4 = sg.Polygon([[243, 2759], [242, 2759], [242, 2760], [243, 2760], [243, 2759]])\n",
    "#r5 = sg.Polygon([[241, 2759], [241, 2760], [242, 2759], [241, 2759]])\n",
    "\n",
    "new_shape = so.cascaded_union(ls)\n",
    "fig, axs = plt.subplots()\n",
    "axs.set_aspect('equal', 'datalim')\n",
    "\n",
    "for geom in new_shape.geoms:    \n",
    "    xs, ys = geom.exterior.xy    \n",
    "    axs.fill(xs, ys, alpha=1, fc='r', ec='none')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8546be8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select index of simplification possibility\n",
    "INDEX = 6\n",
    "\n",
    "\n",
    "possibility = simplify_possibilities[INDEX]\n",
    "\n",
    "if possibility[0] == 'D-P':\n",
    "    # Simplification function Douglas-Peucker\n",
    "    simplified_coordinates = simplify_coords(coordinates, possibility[1])\n",
    "\n",
    "if possibility[0] == 'V-W':\n",
    "    # Simplification function Visvalingam-Whyatt\n",
    "    simplified_coordinates = simplify_coords_vw(coordinates, possibility[1])\n",
    "\n",
    "old_xs, old_ys = zip(*coordinates)\n",
    "new_xs, new_ys = zip(*simplified_coordinates)\n",
    "\n",
    "print(len(simplified_coordinates))\n",
    "print(len(coordinates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b3102",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lines = []\n",
    "Polygons = []\n",
    "MultiPolygons = []\n",
    "a=0\n",
    "for row in wegdeel_buiten_data:\n",
    "    print(str(a) + \" / \" + str(len(wegdeel_buiten_data)), end=\"\\r\")\n",
    "    a = a + 1\n",
    "    keys = row.keys()\n",
    "    \n",
    "    for key in keys:\n",
    "        for element in row[key]['features']:\n",
    "            \n",
    "            if element['geometry']['type'] == 'LineString': \n",
    "                Lines.append(element['geometry']['coordinates'])\n",
    "            \n",
    "            if element['geometry']['type'] == 'Polygon':\n",
    "                Polygons.append(element['geometry']['coordinates'][0])\n",
    "                \n",
    "            if element['geometry']['type'] == 'MultiPolygon':\n",
    "                if element['geometry']['coordinates']:\n",
    "                    for poly in element['geometry']['coordinates'][0]:\n",
    "                        MultiPolygons.append(poly)\n",
    "    \n",
    "    \n",
    "\n",
    "#test = lvl10_data[0]['spoor.se_fld12_lijngeometrie2d']['features'][0]['geometry']['coordinates']\n",
    "#print(Polygons)\n",
    "\n",
    "#print(len(Lines))\n",
    "print(len(Polygons))\n",
    "#print(len(MultiPolygons))\n",
    "\n",
    "ls = []\n",
    "for a in Polygons:\n",
    "    ls.append(len(a))\n",
    "    \n",
    "pd.DataFrame({'lengths':Counter(ls).keys(),\n",
    "              'freq':Counter(ls).values()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d093f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "a=0\n",
    "\n",
    "for element in Polygons[:100]:\n",
    "    results_dict = {}\n",
    "    poly1 = geometry.Polygon(element)\n",
    "    results = []\n",
    "    \n",
    "    for possibility in simplify_possibilities:\n",
    "        \n",
    "        if possibility[0] == 'D-P':\n",
    "            # Simplification function Douglas-Peucker\n",
    "            time_start = time()\n",
    "            simplified_coordinates = simplify_coords(element, possibility[1])\n",
    "            time_end = time()\n",
    "            process_time = time_end - time_start\n",
    "\n",
    "        if possibility[0] == 'V-W':\n",
    "            # Simplification function Visvalingam-Whyatt\n",
    "            time_start = time()\n",
    "            simplified_coordinates = simplify_coords_vw(element, possibility[1])\n",
    "            time_end = time()\n",
    "            process_time = time_end - time_start\n",
    "        \n",
    "        \n",
    "        if len(simplified_coordinates) >= 3:\n",
    "            poly2 = geometry.Polygon(simplified_coordinates)\n",
    "            #length_deficit = (poly2.length - poly1.length) / poly1.length\n",
    "        \n",
    "            # If the length deficit of the polygon is smaller(greater) than the provided MAX_LENGTH_DEFICIT, \n",
    "            # the score gets saved\n",
    "            #if length_deficit > MAX_LENGTH_DEFICIT:\n",
    "            \n",
    "            #if length_deficit == 0:\n",
    "            #    score = ScoreFormula(len(element[0]), len(simplified_coordinates), process_time)\n",
    "            #    results.append(score)\n",
    "            #    continue\n",
    "                \n",
    "            #try:\n",
    "            #    if CheckSameIntersections(element[0], simplified_coordinates, grid, ROUNDING) > MIN_INTERSECTIONS_PERC:\n",
    "            #        score = ScoreFormula(len(element[0]), len(simplified_coordinates), process_time)\n",
    "            #        results.append(score)\n",
    "            #except Exception:\n",
    "            #    continue\n",
    "            \n",
    "            if np.isnan(check_pixel_similarity(element, simplified_coordinates, 17)) == True:\n",
    "                results.append('Remove')\n",
    "                break\n",
    "                \n",
    "                \n",
    "            if check_pixel_similarity(element, simplified_coordinates, 17) == 1:\n",
    "                score = ScoreFormula(len(element), len(simplified_coordinates), process_time)\n",
    "                results.append(score)\n",
    "        \n",
    "    results_dict['index'] = a\n",
    "    results_dict['algorithm'] = results.index(max(results))\n",
    "    results_list.append(results_dict)\n",
    "    a = a + 1\n",
    "    \n",
    "results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_INDEX = 85\n",
    "\n",
    "algorithm = simplify_possibilities[results_list[RESULTS_INDEX]['algorithm']]\n",
    "print(algorithm)\n",
    "points = len(Polygons[results_list[RESULTS_INDEX]['index']])\n",
    "o_xs, o_ys = zip(*Polygons[results_list[RESULTS_INDEX]['index']])\n",
    "#geometry.Polygon(Polygons[results_list[RESULTS_INDEX]['index']])\n",
    "\n",
    "if algorithm[0] == 'D-P':\n",
    "    simplified_poly = geometry.Polygon(simplify_coords(Polygons[results_list[RESULTS_INDEX]['index']], algorithm[1]))\n",
    "    simplified_points = len(simplify_coords(Polygons[results_list[RESULTS_INDEX]['index']], algorithm[1]))\n",
    "    s_xs, s_ys = zip(*simplify_coords(Polygons[results_list[RESULTS_INDEX]['index']], algorithm[1]))\n",
    "    \n",
    "if algorithm[0] == 'V-W':\n",
    "    simplified_poly = geometry.Polygon(simplify_coords_vw(Polygons[results_list[RESULTS_INDEX]['index']], algorithm[1]))\n",
    "    simplified_points = len(simplify_coords_vw(Polygons[results_list[RESULTS_INDEX]['index']], algorithm[1]))\n",
    "    s_xs, s_ys = zip(*simplify_coords_vw(Polygons[results_list[RESULTS_INDEX]['index']], algorithm[1]))\n",
    "\n",
    "print(str(simplified_points) + \" / \" + str(points))\n",
    "geometry.Polygon(Polygons[results_list[RESULTS_INDEX]['index']])\n",
    "simplified_poly    \n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(o_xs,o_ys)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(s_xs,s_ys)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be36ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "99*5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacf5798",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(check_pixel_similarity(Polygons[4], Polygons[0], 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3be7f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = geometry.Polygon(Polygons[5])\n",
    "simplified = geometry.Polygon(simplify_coords(Polygons[62],0.5))\n",
    "simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c88a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axis = plt.subplots(3, 3)\n",
    "\n",
    "axis[0,0].imshow(check_pixel_similarity(Polygons[8], Polygons[0], 20))\n",
    "axis[0,1].imshow(check_pixel_similarity(Polygons[2], Polygons[0], 19))\n",
    "axis[0,2].imshow(check_pixel_similarity(Polygons[4], Polygons[0], 17))\n",
    "\n",
    "axis[1,0].imshow(check_pixel_similarity(Polygons[10], Polygons[0], 19))\n",
    "axis[1,1].imshow(check_pixel_similarity(Polygons[12], Polygons[0], 18))\n",
    "axis[1,2].imshow(check_pixel_similarity(Polygons[16], Polygons[0], 18))\n",
    "\n",
    "axis[2,0].imshow(check_pixel_similarity(Polygons[32], Polygons[0], 19))\n",
    "axis[2,1].imshow(check_pixel_similarity(Polygons[26], Polygons[0], 18))\n",
    "axis[2,2].imshow(check_pixel_similarity(Polygons[24], Polygons[0], 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c218b87a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
