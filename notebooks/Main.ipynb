{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8923376",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f38aa862",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'simplification'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cbf63b02a12f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mshapely\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeometry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m from simplification.cutil import (\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0msimplify_coords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0msimplify_coords_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'simplification'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "#import mapbox_vector_tile\n",
    "from time import time\n",
    "import operator\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import argmax\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from shapely import geometry \n",
    "from PIL import Image, ImageDraw\n",
    "from simplification.cutil import (\n",
    "    simplify_coords,\n",
    "    simplify_coords_idx,\n",
    "    simplify_coords_vw,\n",
    "    simplify_coords_vw_idx,\n",
    "    simplify_coords_vwp,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fba032c",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5f8acaff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_connection(db_file):\n",
    "    \"\"\" create a database connection to a SQLite database \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        print(conn)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    \n",
    "    return conn\n",
    "\n",
    "def PolyArea(x,y):\n",
    "    return 0.5*np.abs(np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)))\n",
    "\n",
    "def ScoreFormula(old_number_of_datapoints, new_number_of_datapoints, processing_time):\n",
    "    return (1 - (new_number_of_datapoints / old_number_of_datapoints)) * (1 - processing_time)\n",
    "\n",
    "\n",
    "def ScaleFactor(all_geometries):\n",
    "    b_list = []\n",
    "    \n",
    "    for geometries in all_geometries:\n",
    "        \n",
    "        polygon = geometry.Polygon(geometries)\n",
    "        centroid = np.array(polygon.centroid)\n",
    "        coordinates = np.vstack(geometries)\n",
    "        \n",
    "        b = coordinates - centroid\n",
    "        b_min = np.min(b)\n",
    "        b_max = np.max(b)\n",
    "        b_list.append(b_min)\n",
    "        b_list.append(b_max)\n",
    "        \n",
    "    return np.std(b_list)\n",
    "    \n",
    "def Normalize_Geometry(coordinates1, scale_factor):\n",
    "    polygon = geometry.Polygon(coordinates1)\n",
    "    centroid = np.array(polygon.centroid)\n",
    "    coordinates2 = np.vstack(coordinates1)\n",
    "    \n",
    "    return (coordinates2 - centroid) / scale_factor\n",
    "\n",
    "def Add_One_Hot(normalized_geometry):\n",
    "    normalized_geometry = np.insert(normalized_geometry, 2, 1, axis=1)\n",
    "    normalized_geometry = np.insert(normalized_geometry, 3, 0, axis=1)\n",
    "    normalized_geometry = np.insert(normalized_geometry, 4, 0, axis=1)\n",
    "    normalized_geometry[len(normalized_geometry)-1,2] = 0\n",
    "    normalized_geometry[len(normalized_geometry)-1,4] = 1\n",
    "    \n",
    "    return normalized_geometry\n",
    "\n",
    "def Add_Zero_Padding(one_hotted_geometry, max_length):\n",
    "    boundary = max_length - len(one_hotted_geometry)\n",
    "    zero_matrix = np.zeros([boundary,len(one_hotted_geometry[0])])\n",
    "    return np.append(one_hotted_geometry, zero_matrix, axis=0)\n",
    "\n",
    "def moment(xy, p, q):\n",
    "    xy = np.asarray(xy)\n",
    "    x = xy[:, 0]\n",
    "    y = xy[:, 1]\n",
    "    x = (x**p) * (x != 0)\n",
    "    y = (y**q) * (y != 0)\n",
    "    M = (x * y).sum(-1)\n",
    "    return torch.tensor(M)\n",
    "\n",
    "def c_mass(xy):\n",
    "    xy = np.asarray(xy)\n",
    "    mass = moment(xy, 0, 0)\n",
    "    mx = moment(xy, 1, 0) / mass\n",
    "    my = moment(xy, 0, 1) / mass\n",
    "    return [mx,my]\n",
    "\n",
    "def mu(xy, p, q):\n",
    "    xy = np.asarray(xy)\n",
    "    m = c_mass(xy)\n",
    "    x = xy[:, 0]\n",
    "    y = xy[:, 1]\n",
    "    x = ((x - m[0])**p) * (x != 0)\n",
    "    y = ((y - m[1])**q) * (y != 0)\n",
    "    M = (x * y).sum(-1)\n",
    "    return M\n",
    "\n",
    "def scale_factor_calculation(xy):\n",
    "    mu_list = [mu(i,0,0) for i in xy]\n",
    "    return sum(mu_list) / len(mu_list)\n",
    "        \n",
    "\n",
    "def scale_factor_apply(xy):\n",
    "    xy = torch.Tensor(xy)\n",
    "    return torch.sqrt((moment(xy,2,0) + moment(xy, 0, 2))/10000000000)\n",
    "\n",
    "def canonical_transformation(xy):\n",
    "#   translation\n",
    "    xy = torch.Tensor(xy)\n",
    "    m = torch.Tensor(c_mass(xy))\n",
    "    x = xy - m.view(1, 2) * (xy[:, 0] != 0).view(-1, 1)\n",
    "\n",
    "#   scale\n",
    "    scale = scale_factor_apply(xy)\n",
    "    x = x / scale\n",
    "    \n",
    "    \n",
    "#   rotation\n",
    "    m_20 = moment(x, 2, 0)\n",
    "    m_02 = moment(x, 0, 2)\n",
    "    m_11 = moment(x, 1, 1)\n",
    "    \n",
    "    angle = np.arctan2(2 * m_11, m_20 - m_02) / 2.0\n",
    "#     return angle\n",
    "    if angle < 0:\n",
    "        angle = np.pi + angle # this is a bad solution\n",
    "        # we need to analyze m_30, m_21, m_12, m_03 to check for flip symmetry\n",
    "    #print(angle*180/np.pi)\n",
    "    \n",
    "    M = torch.Tensor([\n",
    "        [np.cos(angle), np.sin(angle)],\n",
    "        [np.sin(-angle), np.cos(angle)]\n",
    "    ])\n",
    "    x = (M @ x.T).T[None]\n",
    "    \n",
    "    \n",
    "    return x\n",
    "\n",
    "def getAngle(a, b, c):\n",
    "    ang = math.degrees(math.atan2(c[1]-b[1], c[0]-b[0]) - math.atan2(a[1]-b[1], a[0]-b[0]))\n",
    "    return ang + 360 if ang < 0 else ang\n",
    "\n",
    "def polygon_properties(xy):\n",
    "    length = geometry.Polygon(xy).length\n",
    "    points = len(xy)\n",
    "    \n",
    "    b=1\n",
    "    points_distance = []\n",
    "    for coord in xy[:-2]:\n",
    "        points_distance.append(geometry.LineString([coord,xy[b]]).length)\n",
    "        b += 1 \n",
    "    points_distance = pd.DataFrame(points_distance)\n",
    "        \n",
    "    b=1\n",
    "    c=2\n",
    "    angles = []\n",
    "    for coord in xy[:-3]:\n",
    "        angles.append(getAngle(coord, xy[b], xy[c]))\n",
    "        b+=1\n",
    "        c+=1\n",
    "    angles = pd.DataFrame(angles)\n",
    "    \n",
    "    # [number of points, length, average PD, std PD, min PD, max PD, average angle, std angle, min angle, max angle]\n",
    "    return [points, length, points_distance.describe()[0][1], points_distance.describe()[0][2],\n",
    "           points_distance.describe()[0][3], points_distance.describe()[0][7], angles.describe()[0][1], \n",
    "            angles.describe()[0][2], angles.describe()[0][3], angles.describe()[0][7]]\n",
    "\n",
    "ScoreFormula(50,25,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3130d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateGrid(poly, dx, dy):\n",
    "    \n",
    "    x_ls = []\n",
    "    y_ls = []\n",
    "\n",
    "    for a in poly:\n",
    "        x_ls.append(a[0])\n",
    "    for a in poly:\n",
    "        y_ls.append(a[1])\n",
    "        \n",
    "    minx = min(x_ls)\n",
    "    maxx = max(x_ls)\n",
    "    miny = min(y_ls)\n",
    "    maxy = max(y_ls)\n",
    "\n",
    "    nx = int(math.ceil(abs(maxx - minx)/dx))\n",
    "    ny = int(math.ceil(abs(maxy - miny)/dy))\n",
    "\n",
    "    grid = []       \n",
    "    for i in range(ny):   \n",
    "        grid.append(geometry.LineString([[minx,max(maxy-dy*i,miny)], [maxx, max(maxy-dy*i,miny)]]))\n",
    "\n",
    "    for j in range(nx):\n",
    "        grid.append(geometry.LineString([[min(minx+dx*j,maxx), maxy], [min(minx+dx*j,maxx), miny]]))\n",
    "    \n",
    "    return grid\n",
    "    \n",
    "def CheckSameIntersections(poly, simplified_coords, grid, ROUNDING):\n",
    "    \n",
    "    original = geometry.Polygon(poly)\n",
    "    simplified = geometry.Polygon(simplified_coords)\n",
    "\n",
    "    o_ls = []\n",
    "    s_ls = []\n",
    "    for line in grid:\n",
    "        x = original.intersection(line)\n",
    "        y = simplified.intersection(line)\n",
    "        if x:\n",
    "            if x.geom_type == 'Point':\n",
    "                o_ls.append(hash(tuple([round(x.coords[0][0],ROUNDING), round(x.coords[0][1],ROUNDING)])))\n",
    "            if x.geom_type == 'LineString':\n",
    "                for xy in x.coords:\n",
    "                    o_ls.append(hash(tuple([round(xy[0],ROUNDING), round(xy[1],ROUNDING)])))\n",
    "    \n",
    "        if y:\n",
    "            if y.geom_type == 'Point':\n",
    "                s_ls.append(hash(tuple([round(y.coords[0][0],ROUNDING), round(y.coords[0][1],ROUNDING)])))\n",
    "            if y.geom_type == 'LineString':\n",
    "                for xy in y.coords:\n",
    "                    s_ls.append(hash(tuple([round(xy[0],ROUNDING), round(xy[1],ROUNDING)])))\n",
    "        \n",
    "    return len(list(set(o_ls).intersection(s_ls))) / len(set(o_ls))\n",
    "\n",
    "    \n",
    "def alter_by_zoom(poly, zoom):\n",
    "\n",
    "    mpp = {\n",
    "    '0' : 156543,\n",
    "    '1' : 78271.5,\n",
    "    '2' : 39135.8,\n",
    "    '3' : 19567.88,\n",
    "    '4' : 9783.94,\n",
    "    '5' : 4891.97,\n",
    "    '6' : 2445.98,\n",
    "    '7' : 1222.99,\n",
    "    '8' : 611.5,\n",
    "    '9' : 305.75,\n",
    "    '10' : 152.87,\n",
    "    '11' : 76.44,\n",
    "    '12' : 38.219,\n",
    "    '13' : 19.109,\n",
    "    '14' : 9.555,\n",
    "    '15' : 4.777,\n",
    "    '16' : 2.3887,\n",
    "    '17' : 1.1943,\n",
    "    '18' : 0.5972,\n",
    "    '19' : 0.2986,\n",
    "    '20' : 0.14929,\n",
    "    '21' : 0.074646,\n",
    "    '22' : 0.037323\n",
    "    }\n",
    "    return (np.array(poly) / mpp[str(zoom)]).tolist()\n",
    "\n",
    "\n",
    "def check_pixel_similarity(original_coords, simplified_coords, zoom):\n",
    "    \n",
    "    poly1 = alter_by_zoom(original_coords, zoom)\n",
    "    poly2 = alter_by_zoom(simplified_coords, zoom)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for a in poly1:\n",
    "        x.append(a[0])\n",
    "        y.append(a[1])\n",
    "    \n",
    "    for a in poly1:\n",
    "        a[0] = a[0] - min(x)\n",
    "        a[1] = a[1] - min(y)\n",
    "    \n",
    "    for a in poly2:\n",
    "        a[0] = a[0] - min(x)\n",
    "        a[1] = a[1] - min(y)\n",
    "    \n",
    "    width = int(max(x) - min(x))\n",
    "    height = int(max(y) - min(y))\n",
    "\n",
    "    poly1 = [tuple(x) for x in poly1]\n",
    "    poly2 = [tuple(x) for x in poly2]\n",
    "\n",
    "    img1 = Image.new('L', (width, height), 0)\n",
    "    ImageDraw.Draw(img1).polygon(poly1, outline=1, fill=0)\n",
    "    mask1 = np.array(img1)\n",
    "    \n",
    "    img2 = Image.new('L', (width, height), 0)\n",
    "    ImageDraw.Draw(img2).polygon(poly2, outline=1, fill=0)\n",
    "    mask2 = np.array(img2)\n",
    "    \n",
    "    return np.sum(mask1 == mask2) / (width*height)\n",
    "    #return mask1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eb1b8b",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fbd72612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waterdeel_export_stedelijk_geometrie.json\n",
      "wegdeel_export_buitengebied_geometrie.json\n",
      "bag_pand_buitengebeid_export_geometrie.json\n",
      "wegdeel_export_stedelijk_geometrie.json\n",
      "spoor_export_stedelijk_geometrie.json\n",
      "waterdeel_export_buitengebied_geometrie.json\n",
      "bag_pand_stedelijk_export_geometrie.json\n",
      "spoor_export_buitengebied_geometrie.json\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/davemeijdam/Documents/Data Science/Master/Master Thesis/Data/Sample_data_03_05/'\n",
    "Polygons = []\n",
    "Types = []\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    if \"geometrie.\" in filename:\n",
    "        print(filename)\n",
    "        \n",
    "        f = open(str(path + filename))\n",
    "        jsondata = json.load(f)\n",
    "        \n",
    "        \n",
    "\n",
    "        for a in jsondata['features']:\n",
    "            if len(a['geometry']['coordinates']) == 1:\n",
    "                Polygons.append(a['geometry']['coordinates'][0])\n",
    "                Types.append(a['geometry']['type'])\n",
    "            if a['geometry']['type'] == 'LineString':\n",
    "                Polygons.append(a['geometry']['coordinates'])\n",
    "                Types.append(a['geometry']['type'])\n",
    "            else:\n",
    "                for b in a['geometry']['coordinates']:\n",
    "                    Polygons.append(b)\n",
    "                    Types.append(a['geometry']['type'])\n",
    "            \n",
    "geometry_df = pd.DataFrame({'geometry':Polygons,\n",
    "                            'type':Types})\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#f = open('/Users/davemeijdam/Documents/Data Science/Master/Master Thesis/Data/Sample_data_03_05/spoor_export_buitengebied_geometrie.json')\n",
    "#wegdeeljson = json.load(f)\n",
    "#wegdeeljson\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8911f217",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a2e8ec16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simplification Possibilities\n",
    "simplify_possibilities = [['D-P', 0], ['D-P', 0.5], ['D-P', 0.1], ['D-P', 0.05], ['D-P', 0.01], ['D-P', 0.005], \n",
    "                          ['D-P', 0.001], ['V-W', 0.5], ['V-W', 0.1], ['V-W', 0.05], ['V-W', 0.01], \n",
    "                          ['V-W', 0.005]]\n",
    "\n",
    "#simplify_possibilities = [['D-P', 0], ['D-P', 0.5], ['D-P', 0.1], ['D-P', 0.05], ['D-P', 0.01], ['D-P', 0.005], \n",
    "#                          ['D-P', 0.001], ['V-W', 0.5], ['V-W', 0.1], ['V-W', 0.05], ['V-W', 0.01], \n",
    "#                          ['V-W', 0.005], ['V-W', 0.001], ['V-W', 0.0005], ['V-W', 0.0001], ['V-W', 0.00005]]\n",
    "\n",
    "# Polygon length evaluation\n",
    "MAX_LENGTH_DEFICIT = -0.1\n",
    "\n",
    "# Grid\n",
    "dx = 1\n",
    "dy = 1\n",
    "ROUNDING = 1\n",
    "\n",
    "MIN_INTERSECTIONS_PERC = 0.75\n",
    "\n",
    "len(simplify_possibilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8fe237",
   "metadata": {},
   "source": [
    "# Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fa2b1b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294580"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Polygons = list(geometry_df['geometry'][geometry_df['type'] == 'Polygon'])\n",
    "Lines = list(geometry_df['geometry'][geometry_df['type'] == 'LineString'])\n",
    "\n",
    "Polygons_list = []\n",
    "for element in Polygons:\n",
    "    if len(element) < 100:\n",
    "        Polygons_list.append(element)\n",
    "Polygons = Polygons_list\n",
    "len(Polygons)\n",
    "#len(Lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f83de39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale Factor done\n",
      "Sorted the Polygons\n",
      "6179 / 250000\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/infoviz/lib/python3.7/site-packages/ipykernel_launcher.py:118: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/opt/anaconda3/envs/infoviz/lib/python3.7/site-packages/ipykernel_launcher.py:118: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249999 / 250000\r"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "length_list = []\n",
    "Polygons_sample = random.sample(Polygons, 250000)\n",
    "#scale_factor = scale_factor_calculation(Polygons_sample)\n",
    "print(\"Scale Factor done\")\n",
    "\n",
    "\n",
    "# Decide order from longest polygon to smallest polygon\n",
    "for row in Polygons_sample:\n",
    "\n",
    "    length_list.append([row, len(row)])\n",
    "\n",
    "length_list.sort(key=operator.itemgetter(1), reverse=True)\n",
    "print(\"Sorted the Polygons\")\n",
    "a=0\n",
    "\n",
    "y_ls = []\n",
    "    \n",
    "for element in length_list:\n",
    "    print(str(a) + \" / \" + str(len(length_list)), end=\"\\r\")\n",
    "    a = a + 1\n",
    "    results_dict = {}\n",
    "    poly1 = geometry.Polygon(element[0])\n",
    "    results = []\n",
    "    process_time_tensor = torch.zeros(len(simplify_possibilities)+1)\n",
    "    datasize_tensor = torch.zeros(len(simplify_possibilities)+1)\n",
    "    variance_penalty_tensor = torch.ones(len(simplify_possibilities)+1)\n",
    "    \n",
    "    i=0\n",
    "    for possibility in simplify_possibilities:\n",
    "        \n",
    "\n",
    "        if possibility[0] == 'D-P':\n",
    "            # Simplification function Douglas-Peucker\n",
    "            time_start = time()\n",
    "            simplified_coordinates = simplify_coords(element[0], possibility[1])\n",
    "            time_end = time()\n",
    "            process_time = time_end - time_start\n",
    "\n",
    "        if possibility[0] == 'V-W':\n",
    "            # Simplification function Visvalingam-Whyatt\n",
    "            time_start = time()\n",
    "            simplified_coordinates = simplify_coords_vw(element[0], possibility[1])\n",
    "            time_end = time()\n",
    "            process_time = time_end - time_start\n",
    "            \n",
    "        process_time_tensor[i] = torch.tensor(process_time * 1000)\n",
    "        datasize_tensor[i] = torch.tensor(len(simplified_coordinates) / len(element[0]))\n",
    "        \n",
    "        \n",
    "        if len(simplified_coordinates) >= 3:\n",
    "            poly2 = geometry.Polygon(simplified_coordinates)\n",
    "            \n",
    "            if np.isnan(check_pixel_similarity(element[0], simplified_coordinates, 17)) == True:\n",
    "                results.append('Remove')\n",
    "                variance_penalty_tensor[len(simplify_possibilities)] = torch.tensor(0)\n",
    "                \n",
    "                \n",
    "            if check_pixel_similarity(element[0], simplified_coordinates, 17) == 1:\n",
    "                score = ScoreFormula(len(element[0]), len(simplified_coordinates), process_time)\n",
    "                #results.append(score)\n",
    "                dicti = {\"i\": i, \"score\": score}\n",
    "                results.append(dicti)\n",
    "                variance_penalty_tensor[i] = torch.tensor(0)\n",
    "        \n",
    "        \n",
    "        i = i + 1\n",
    "    y_tensor = torch.Tensor(process_time_tensor * datasize_tensor + variance_penalty_tensor)\n",
    "    y_ls.append(y_tensor)\n",
    "    \n",
    "    #results_dict['polygon'] = Add_Zero_Padding(element[0], len(length_list[0][0]))\n",
    "    results_dict['polygon'] = Add_Zero_Padding(canonical_transformation(element[0])[0], len(length_list[0][0]))\n",
    "    results_dict['properties'] = polygon_properties(element[0])\n",
    "    \n",
    "    if results[0] == 'Remove':\n",
    "        results_dict['algorithm_top1'] = len(simplify_possibilities)\n",
    "        \n",
    "    else:\n",
    "        if len(results) >= 1:\n",
    "        \n",
    "            results_df = pd.DataFrame(results).sort_values('score', ascending = False)\n",
    "            results_dict['algorithm_top1'] = results_df['i'].iloc[0]\n",
    "        \n",
    "        if len(results) >= 3:\n",
    "            results_df = pd.DataFrame(results).sort_values('score', ascending = False)\n",
    "            results_dict['algorithm_top3'] = list(results_df['i'][0:3])\n",
    "        \n",
    "        if len(results) >= 5:\n",
    "            results_df = pd.DataFrame(results).sort_values('score', ascending = False)\n",
    "            results_dict['algorithm_top5'] = list(results_df['i'][0:5])\n",
    "    \n",
    "    results_dict['algorithm_all'] = results_df['i']\n",
    "        \n",
    "    results_list.append(results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3faf29",
   "metadata": {},
   "source": [
    "# Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a0f97692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Labels and Normalized Data\n",
    "#pickle.dump( results_list, open( \"/Users/davemeijdam/Documents/Data Science/Master/Master Thesis/Scripts/data/temp/results_list_NoNorm.p\", \"wb\" ) )\n",
    "\n",
    "#pickle.dump( results_list, open( \"/Users/davemeijdam/Documents/Data Science/Master/Master Thesis/Scripts/data/temp/results_list.p\", \"wb\" ) )\n",
    "pickle.dump( results_list, open( \"/Users/davemeijdam/Documents/Data Science/Master/Master Thesis/Scripts/data/temp/results_list2.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fa0665",
   "metadata": {},
   "source": [
    "# Data Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c1ade7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     165574\n",
       "7      17095\n",
       "8      16045\n",
       "10     10823\n",
       "9       9722\n",
       "1       8859\n",
       "11      8816\n",
       "3       3307\n",
       "2       3294\n",
       "4       2135\n",
       "6       2001\n",
       "5       1543\n",
       "12       786\n",
       "Name: algorithm_top1, dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_list = pickle.load( open( \"/Users/davemeijdam/Documents/Data Science/Master/Master Thesis/Scripts/data/temp/results_list.p\", \"rb\" ) )\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "results_df['algorithm_top1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "acb71142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polygon</th>\n",
       "      <th>properties</th>\n",
       "      <th>algorithm_top1</th>\n",
       "      <th>algorithm_top3</th>\n",
       "      <th>algorithm_top5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.799656093120575, -0.0946083813905716], [0....</td>\n",
       "      <td>[99, 2178.9610140710192, 22.450495558452648, 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-0.3245452344417572, -0.011429402977228165],...</td>\n",
       "      <td>[99, 874.944064247078, 9.016111931462284, 31.4...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-0.006566339172422886, 0.002158563816919923]...</td>\n",
       "      <td>[99, 238.95577812211855, 2.4393933891310806, 4...</td>\n",
       "      <td>10</td>\n",
       "      <td>[10, 11, 4]</td>\n",
       "      <td>[10, 11, 4, 5, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.07178658992052078, -0.001564469188451767],...</td>\n",
       "      <td>[99, 388.7001010597434, 3.916680848840221, 4.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.025732271373271942, -0.000614214688539505]...</td>\n",
       "      <td>[99, 311.63483563113834, 3.0551961592603374, 3...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249995</th>\n",
       "      <td>[[0.00048117226106114686, 0.001283125719055533...</td>\n",
       "      <td>[4, 1.5893729078600223, 0.6204029687897032, 0....</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249996</th>\n",
       "      <td>[[0.017680831253528595, 0.0130640659481287], [...</td>\n",
       "      <td>[4, 24.75422324197516, 9.954300010573544, 0.27...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[0, 1, 2, 3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249997</th>\n",
       "      <td>[[-0.0009137333836406469, 0.0], [-0.0009137333...</td>\n",
       "      <td>[4, 1.4463090708624708, 0.363968358041024, 0.4...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249998</th>\n",
       "      <td>[[0.04203283414244652, -0.0007698573172092438]...</td>\n",
       "      <td>[4, 33.20623607278932, 8.304872996199604, 2.16...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 2, 3]</td>\n",
       "      <td>[0, 2, 3, 4, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249999</th>\n",
       "      <td>[[0.013471527025103569, 0.002519744448363781],...</td>\n",
       "      <td>[4, 11.212777727688955, 4.010673233351945, 0.9...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[0, 1, 2, 3, 4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  polygon  \\\n",
       "0       [[0.799656093120575, -0.0946083813905716], [0....   \n",
       "1       [[-0.3245452344417572, -0.011429402977228165],...   \n",
       "2       [[-0.006566339172422886, 0.002158563816919923]...   \n",
       "3       [[0.07178658992052078, -0.001564469188451767],...   \n",
       "4       [[0.025732271373271942, -0.000614214688539505]...   \n",
       "...                                                   ...   \n",
       "249995  [[0.00048117226106114686, 0.001283125719055533...   \n",
       "249996  [[0.017680831253528595, 0.0130640659481287], [...   \n",
       "249997  [[-0.0009137333836406469, 0.0], [-0.0009137333...   \n",
       "249998  [[0.04203283414244652, -0.0007698573172092438]...   \n",
       "249999  [[0.013471527025103569, 0.002519744448363781],...   \n",
       "\n",
       "                                               properties  algorithm_top1  \\\n",
       "0       [99, 2178.9610140710192, 22.450495558452648, 1...               0   \n",
       "1       [99, 874.944064247078, 9.016111931462284, 31.4...               0   \n",
       "2       [99, 238.95577812211855, 2.4393933891310806, 4...              10   \n",
       "3       [99, 388.7001010597434, 3.916680848840221, 4.0...               0   \n",
       "4       [99, 311.63483563113834, 3.0551961592603374, 3...               0   \n",
       "...                                                   ...             ...   \n",
       "249995  [4, 1.5893729078600223, 0.6204029687897032, 0....              12   \n",
       "249996  [4, 24.75422324197516, 9.954300010573544, 0.27...               0   \n",
       "249997  [4, 1.4463090708624708, 0.363968358041024, 0.4...              12   \n",
       "249998  [4, 33.20623607278932, 8.304872996199604, 2.16...               0   \n",
       "249999  [4, 11.212777727688955, 4.010673233351945, 0.9...               0   \n",
       "\n",
       "       algorithm_top3     algorithm_top5  \n",
       "0                   0                  0  \n",
       "1                   0                  0  \n",
       "2         [10, 11, 4]  [10, 11, 4, 5, 6]  \n",
       "3                   0                  0  \n",
       "4                   0                  0  \n",
       "...               ...                ...  \n",
       "249995             12                 12  \n",
       "249996      [0, 1, 2]    [0, 1, 2, 3, 4]  \n",
       "249997             12                 12  \n",
       "249998      [0, 2, 3]    [0, 2, 3, 4, 5]  \n",
       "249999      [0, 1, 2]    [0, 1, 2, 3, 4]  \n",
       "\n",
       "[250000 rows x 5 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in results_df.iterrows():\n",
    "    if isinstance(row['algorithm_top5'], list) == False:\n",
    "        if isinstance(row['algorithm_top3'], list) == False:\n",
    "            results_df.at[index,'algorithm_top5'] = row['algorithm_top1']\n",
    "        else:\n",
    "            results_df.at[index,'algorithm_top5'] = row['algorithm_top3']\n",
    "            \n",
    "    if isinstance(row['algorithm_top3'], list) == False:\n",
    "        results_df.at[index,'algorithm_top3'] = row['algorithm_top1']\n",
    "    \n",
    "results_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ddce86c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11    8816\n",
       "10    5000\n",
       "9     5000\n",
       "8     5000\n",
       "7     5000\n",
       "1     5000\n",
       "0     5000\n",
       "3     3307\n",
       "2     3294\n",
       "4     2135\n",
       "6     2001\n",
       "5     1543\n",
       "12     786\n",
       "Name: algorithm_top1, dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_majority = results_df[results_df.algorithm_top1 == 0]\n",
    "df_minority = results_df[results_df.algorithm_top1 != 0]\n",
    "\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                  replace=False,\n",
    "                                  n_samples=5000,\n",
    "                                   #n_samples=500,\n",
    "                                  random_state=123)\n",
    "\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "df_majority = df_downsampled[df_downsampled.algorithm_top1 == 7]\n",
    "df_minority = df_downsampled[df_downsampled.algorithm_top1 != 7]\n",
    "\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                  replace=False,\n",
    "                                  n_samples=5000,\n",
    "                                   #n_samples=500,\n",
    "                                  random_state=123)\n",
    "\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "df_majority = df_downsampled[df_downsampled.algorithm_top1 == 8]\n",
    "df_minority = df_downsampled[df_downsampled.algorithm_top1 != 8]\n",
    "\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                  replace=False,\n",
    "                                  n_samples=5000,\n",
    "                                   #n_samples=500,\n",
    "                                  random_state=123)\n",
    "\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "df_majority = df_downsampled[df_downsampled.algorithm_top1 == 10]\n",
    "df_minority = df_downsampled[df_downsampled.algorithm_top1 != 10]\n",
    "\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                  replace=False,\n",
    "                                  n_samples=5000,\n",
    "                                   #n_samples=500,\n",
    "                                  random_state=123)\n",
    "\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "df_majority = df_downsampled[df_downsampled.algorithm_top1 == 9]\n",
    "df_minority = df_downsampled[df_downsampled.algorithm_top1 != 9]\n",
    "\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                  replace=False,\n",
    "                                  n_samples=5000,\n",
    "                                  # n_samples=500,\n",
    "                                  random_state=123)\n",
    "\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "df_majority = df_downsampled[df_downsampled.algorithm_top1 == 1]\n",
    "df_minority = df_downsampled[df_downsampled.algorithm_top1 != 1]\n",
    "\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                  replace=False,\n",
    "                                  n_samples=5000,\n",
    "                                  # n_samples=500,\n",
    "                                  random_state=123)\n",
    "\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "results_list = df_downsampled.to_dict('records')\n",
    "\n",
    "df_downsampled['algorithm_top1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "71535ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot(y,length):\n",
    "    output = []\n",
    "    for i in y:\n",
    "        array = np.zeros(length)\n",
    "        if isinstance(i, int) == False:\n",
    "            for j in i:\n",
    "                array[j] = 1\n",
    "        else:\n",
    "            array[i] = 1\n",
    "                \n",
    "        output.append(array)\n",
    "    return np.array(output)\n",
    "\n",
    "def make_one_one_hot(y,length):\n",
    "    array = np.zeros(length)\n",
    "    array[y] = 1\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ba0cdb",
   "metadata": {},
   "source": [
    "## Data processing top1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1874e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for element in results_list:\n",
    "    \n",
    "    X.append(element['polygon'])\n",
    "    y.append(element['algorithm_top1'])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "y = np.nan_to_num(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "y_onehottrain = to_categorical(y_train)\n",
    "train_tensor = []\n",
    "b=0\n",
    "for a in X_train:\n",
    "    d = y_onehottrain[b]\n",
    "    c = y_train[b]\n",
    "    train_tensor.append([a,c,d]) \n",
    "    b = b+1\n",
    "\n",
    "y_onehottest = to_categorical(y_test)\n",
    "test_tensor = []\n",
    "b=0\n",
    "for a in X_test:\n",
    "    d = y_onehottest[b]\n",
    "    c = y_test[b]\n",
    "    test_tensor.append([a,c,d])\n",
    "    b = b+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1be5a6",
   "metadata": {},
   "source": [
    "## Data processing top3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83819756",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for element in results_list:\n",
    "    \n",
    "    X.append(element['polygon'])\n",
    "    y.append(element['algorithm_top3'])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "y = np.nan_to_num(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "y_onehottrain = make_one_hot(y_train, 13)\n",
    "train_tensor = []\n",
    "b=0\n",
    "for a in X_train:\n",
    "    d = y_onehottrain[b]\n",
    "    if isinstance(y_train[b], list) == True:\n",
    "        c = y_train[b][0]\n",
    "    else:\n",
    "        c = y_train[b]\n",
    "    train_tensor.append([a,c,d]) \n",
    "    b = b+1\n",
    "\n",
    "y_onehottest = make_one_hot(y_test, 13)\n",
    "test_tensor = []\n",
    "b=0\n",
    "for a in X_test:\n",
    "    d = y_onehottest[b]\n",
    "    if isinstance(y_test[b], list) == True:\n",
    "        c = y_test[b][0]\n",
    "    else:\n",
    "        c = y_test[b]\n",
    "    test_tensor.append([a,c,d])\n",
    "    b = b+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa686a0",
   "metadata": {},
   "source": [
    "## Data processing top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "592f90c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for element in results_list:\n",
    "    \n",
    "    X.append(element['polygon'])\n",
    "    y.append(element['algorithm_top5'])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "y = np.nan_to_num(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "y_onehottrain = make_one_hot(y_train, 13)\n",
    "train_tensor = []\n",
    "b=0\n",
    "for a in X_train:\n",
    "    d = y_onehottrain[b]\n",
    "    if isinstance(y_train[b], list) == True:\n",
    "        c = y_train[b][0]\n",
    "    else:\n",
    "        c = y_train[b]\n",
    "    train_tensor.append([a,c,d]) \n",
    "    b = b+1\n",
    "\n",
    "y_onehottest = make_one_hot(y_test, 13)\n",
    "test_tensor = []\n",
    "b=0\n",
    "for a in X_test:\n",
    "    d = y_onehottest[b]\n",
    "    if isinstance(y_test[b], list) == True:\n",
    "        c = y_test[b][0]\n",
    "    else:\n",
    "        c = y_test[b]\n",
    "    test_tensor.append([a,c,d])\n",
    "    b = b+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c8eb5",
   "metadata": {},
   "source": [
    "# Data Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0930fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'keys':list(Counter(y_test).keys()),\n",
    "              'freq':list(Counter(y_test).values())})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25c23f7",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "389a4005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([46693, 198])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MNIST_Polygon(Dataset):\n",
    "    \n",
    "    def __init__(self, tensor, transform=None):\n",
    "        data = [x for x, y, z in tensor]\n",
    "        data = np.asarray(data)\n",
    "        self.data = torch.reshape(torch.from_numpy(data).float(), (data.shape[0], data.shape[1]*data.shape[2]))\n",
    "        self.targets = [torch.tensor(y).long() for x, y, z in tensor]\n",
    "        self.onehot = [z for x, y, z in tensor]\n",
    "                \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "MNIST_Polygon(train_tensor).data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "48b5a288",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 64\n",
    "EPOCHS = 14\n",
    "LR = 0.002\n",
    "LOG_INTERVAL = 10\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(198, 256, bias=False),\n",
    "            #nn.BatchNorm1d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 128, bias=False), \n",
    "            #nn.BatchNorm1d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 13),\n",
    "            nn.Softmax(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.conv(x)\n",
    "        #x = torch.flatten(x, 1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20ae64a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90045863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e1cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymmetryPlusData(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.poly_net = nn.Sequential(\n",
    "            nn.Linear(198, 256, bias=False),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 128, bias=False), \n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(10, 64, bias=False),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 32, bias=False), \n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.BatchNorm1d(160),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(160, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 13),\n",
    "            nn.Softmax(1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, poly, features):\n",
    "        poly = normalize(poly)\n",
    "        y1 = self.poly_net(poly)\n",
    "        y2 = self.feature_net(features)\n",
    "        y = torch.cat([y1, y2], -1)\n",
    "        return self.classifier(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d9d2f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b03a75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aad1a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.softmax(torch.randn(13), 0)\n",
    "comps = 5 + torch.randn(13)\n",
    "comps = comps / comps.sum()\n",
    "times = 100 + torch.randn(13)\n",
    "times = times / times.sum()\n",
    "\n",
    "\n",
    "loss = (0.1 * times + comps + penalty).exp() * x\n",
    "loss = loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb680dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"\n",
    "    Computes the accuracy over the k top predictions for the specified values of k\n",
    "    In top-5 accuracy you give yourself credit for having the right answer\n",
    "    if the right answer appears in your top five guesses.\n",
    "\n",
    "    ref:\n",
    "    - https://pytorch.org/docs/stable/generated/torch.topk.html\n",
    "    - https://discuss.pytorch.org/t/imagenet-example-accuracy-calculation/7840\n",
    "    - https://gist.github.com/weiaicunzai/2a5ae6eac6712c70bde0630f3e76b77b\n",
    "    - https://discuss.pytorch.org/t/top-k-error-calculation/48815/2\n",
    "    - https://stackoverflow.com/questions/59474987/how-to-get-top-k-accuracy-in-semantic-segmentation-using-pytorch\n",
    "\n",
    "    :param output: output is the prediction of the model e.g. scores, logits, raw y_pred before normalization or getting classes\n",
    "    :param target: target is the truth\n",
    "    :param topk: tuple of topk's to compute e.g. (1, 2, 5) computes top 1, top 2 and top 5.\n",
    "    e.g. in top 2 it means you get a +1 if your models's top 2 predictions are in the right label.\n",
    "    So if your model predicts cat, dog (0, 1) and the true label was bird (3) you get zero\n",
    "    but if it were either cat or dog you'd accumulate +1 for that example.\n",
    "    :return: list of topk accuracy [top1st, top2nd, ...] depending on your topk input\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # ---- get the topk most likely labels according to your model\n",
    "        # get the largest k \\in [n_classes] (i.e. the number of most likely probabilities we will use)\n",
    "        maxk = max(topk)  # max number labels we will consider in the right choices for out model\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        # get top maxk indicies that correspond to the most likely probability scores\n",
    "        # (note _ means we don't care about the actual top maxk scores just their corresponding indicies/labels)\n",
    "        _, y_pred = output.topk(k=maxk, dim=1)  # _, [B, n_classes] -> [B, maxk]\n",
    "        y_pred = y_pred.t()  # [B, maxk] -> [maxk, B] Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.\n",
    "\n",
    "        # - get the credit for each example if the models predictions is in maxk values (main crux of code)\n",
    "        # for any example, the model will get credit if it's prediction matches the ground truth\n",
    "        # for each example we compare if the model's best prediction matches the truth. If yes we get an entry of 1.\n",
    "        # if the k'th top answer of the model matches the truth we get 1.\n",
    "        # Note: this for any example in batch we can only ever get 1 match (so we never overestimate accuracy <1)\n",
    "        target_reshaped = target.view(1, -1).expand_as(y_pred)  # [B] -> [B, 1] -> [maxk, B]\n",
    "        # compare every topk's model prediction with the ground truth & give credit if any matches the ground truth\n",
    "        correct = (y_pred == target_reshaped)  # [maxk, B] were for each example we know which topk prediction matched truth\n",
    "        # original: correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        # -- get topk accuracy\n",
    "        list_topk_accs = []  # idx is topk1, topk2, ... etc\n",
    "        for k in topk:\n",
    "            # get tensor of which topk answer was right\n",
    "            ind_which_topk_matched_truth = correct[:k]  # [maxk, B] -> [k, B]\n",
    "            # flatten it to help compute if we got it correct for each example in batch\n",
    "            flattened_indicator_which_topk_matched_truth = ind_which_topk_matched_truth.reshape(-1).float()  # [k, B] -> [kB]\n",
    "            # get if we got it right for any of our top k prediction for each example in batch\n",
    "            tot_correct_topk = flattened_indicator_which_topk_matched_truth.float().sum(dim=0, keepdim=True)  # [kB] -> [1]\n",
    "            # compute topk accuracy - the accuracy of the mode's ability to get it right within it's top k guesses/preds\n",
    "            topk_acc = tot_correct_topk / batch_size  # topk accuracy for entire batch\n",
    "            list_topk_accs.append(topk_acc)\n",
    "        return list_topk_accs  # list of topk accuracies for entire batch [topk1, topk2, ... etc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "627887f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()), end=\"\\r\")\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, results_list, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    a = 0\n",
    "    real = dataset2.onehot\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            #correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            for i in pred:\n",
    "                correct += real[a][i]\n",
    "                a+=1\n",
    "            \n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    results_list.append([epoch, correct / len(test_loader.dataset), test_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e34bbd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [46080/46693 (99%)]\tLoss: 2.510487\n",
      "Test set: Average loss: 2.4797, Accuracy: 1044.0/5189 (20.1%)\n",
      "\n",
      "Train Epoch: 2 [46080/46693 (99%)]\tLoss: 2.396911\n",
      "Test set: Average loss: 2.4498, Accuracy: 1221.0/5189 (23.5%)\n",
      "\n",
      "Train Epoch: 3 [46080/46693 (99%)]\tLoss: 2.404052\n",
      "Test set: Average loss: 2.4423, Accuracy: 1291.0/5189 (24.9%)\n",
      "\n",
      "Train Epoch: 4 [46080/46693 (99%)]\tLoss: 2.400693\n",
      "Test set: Average loss: 2.4397, Accuracy: 1280.0/5189 (24.7%)\n",
      "\n",
      "Train Epoch: 5 [46080/46693 (99%)]\tLoss: 2.368733\n",
      "Test set: Average loss: 2.4395, Accuracy: 1284.0/5189 (24.7%)\n",
      "\n",
      "Train Epoch: 6 [46080/46693 (99%)]\tLoss: 2.409525\n",
      "Test set: Average loss: 2.4361, Accuracy: 1308.0/5189 (25.2%)\n",
      "\n",
      "Train Epoch: 7 [46080/46693 (99%)]\tLoss: 2.458135\n",
      "Test set: Average loss: 2.4355, Accuracy: 1309.0/5189 (25.2%)\n",
      "\n",
      "Train Epoch: 8 [46080/46693 (99%)]\tLoss: 2.429002\n",
      "Test set: Average loss: 2.4355, Accuracy: 1302.0/5189 (25.1%)\n",
      "\n",
      "Train Epoch: 9 [46080/46693 (99%)]\tLoss: 2.463405\n",
      "Test set: Average loss: 2.4359, Accuracy: 1296.0/5189 (25.0%)\n",
      "\n",
      "Train Epoch: 10 [46080/46693 (99%)]\tLoss: 2.430487\n",
      "Test set: Average loss: 2.4338, Accuracy: 1322.0/5189 (25.5%)\n",
      "\n",
      "Train Epoch: 11 [46080/46693 (99%)]\tLoss: 2.406308\n",
      "Test set: Average loss: 2.4313, Accuracy: 1324.0/5189 (25.5%)\n",
      "\n",
      "Train Epoch: 12 [46080/46693 (99%)]\tLoss: 2.408440\n",
      "Test set: Average loss: 2.4323, Accuracy: 1316.0/5189 (25.4%)\n",
      "\n",
      "Train Epoch: 13 [46080/46693 (99%)]\tLoss: 2.474036\n",
      "Test set: Average loss: 2.4300, Accuracy: 1319.0/5189 (25.4%)\n",
      "\n",
      "Train Epoch: 14 [46080/46693 (99%)]\tLoss: 2.520113\n",
      "Test set: Average loss: 2.4295, Accuracy: 1330.0/5189 (25.6%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset1 = MNIST_Polygon(train_tensor)\n",
    "dataset2 = MNIST_Polygon(test_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,batch_size=BATCH_SIZE, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "model = Net().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "results_list = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    test(model, DEVICE, test_loader, results_list, epoch)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831abc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bcb6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_list)\n",
    "results_df.columns = ['epoch', 'accuracy', 'loss']\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b171802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "a = 0\n",
    "for element in results_list:\n",
    "    \n",
    "    X.append(element['properties'])\n",
    "    y.append(element['algorithm_top1'])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X = np.nan_to_num(X)\n",
    "y = np.nan_to_num(y)\n",
    "\n",
    "a=0\n",
    "for a in range(X.shape[1]):\n",
    "    X[:,a] = (X[:,a] - X[:,a].mean()) / X[:,a].std()\n",
    "#print(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "#y_onehottrain = make_one_hot(y_train, 13)\n",
    "y_onehottrain = to_categorical(y_train)\n",
    "train_tensor = []\n",
    "b=0\n",
    "for a in X_train:\n",
    "    d = y_onehottrain[b]\n",
    "    if isinstance(y_train[b], list) == True:\n",
    "        c = y_train[b][0]\n",
    "    else:\n",
    "        c = y_train[b]\n",
    "    train_tensor.append([a,c,d]) \n",
    "    b = b+1\n",
    "\n",
    "#y_onehottest = make_one_hot(y_test, 13)\n",
    "y_onehottest = to_categorical(y_test)\n",
    "test_tensor = []\n",
    "b=0\n",
    "for a in X_test:\n",
    "    d = y_onehottest[b]\n",
    "    if isinstance(y_test[b], list) == True:\n",
    "        c = y_test[b][0]\n",
    "    else:\n",
    "        c = y_test[b]\n",
    "    test_tensor.append([a,c,d])\n",
    "    b = b+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "79730c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([46693, 10])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MNIST_Polygon2(Dataset):\n",
    "    \n",
    "    def __init__(self, tensor, transform=None):\n",
    "        data = [x for x, y, z in tensor]\n",
    "        self.data = torch.Tensor(data).float()\n",
    "        #self.data = torch.reshape(torch.from_numpy(data).float(), (data.shape[0], data.shape[1]*data.shape[2]))\n",
    "        self.targets = [torch.tensor(y).long() for x, y, z in tensor]\n",
    "        self.onehot = [z for x, y, z in tensor]\n",
    "                \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "MNIST_Polygon2(train_tensor).data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b8512270",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 64\n",
    "EPOCHS = 14\n",
    "LR = 0.002\n",
    "LOG_INTERVAL = 10\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(10, 64, bias=False),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 32, bias=False), \n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 13),\n",
    "            nn.Softmax(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.conv(x)\n",
    "        #x = torch.flatten(x, 1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4c9c57a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()), end=\"\\r\")\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, results_list, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    a = 0\n",
    "    real = dataset2.onehot\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            #correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            for i in pred:\n",
    "                correct += real[a][i]\n",
    "                a+=1\n",
    "            \n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    results_list.append([epoch, correct / len(test_loader.dataset), test_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "64d1f158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [46080/46693 (99%)]\tLoss: 2.417198\n",
      "Test set: Average loss: 2.4160, Accuracy: 1402.0/5189 (27.0%)\n",
      "\n",
      "Train Epoch: 2 [46080/46693 (99%)]\tLoss: 2.401640\n",
      "Test set: Average loss: 2.4109, Accuracy: 1412.0/5189 (27.2%)\n",
      "\n",
      "Train Epoch: 3 [46080/46693 (99%)]\tLoss: 2.355218\n",
      "Test set: Average loss: 2.4078, Accuracy: 1432.0/5189 (27.6%)\n",
      "\n",
      "Train Epoch: 4 [46080/46693 (99%)]\tLoss: 2.395122\n",
      "Test set: Average loss: 2.4037, Accuracy: 1467.0/5189 (28.3%)\n",
      "\n",
      "Train Epoch: 5 [46080/46693 (99%)]\tLoss: 2.437517\n",
      "Test set: Average loss: 2.4058, Accuracy: 1430.0/5189 (27.6%)\n",
      "\n",
      "Train Epoch: 6 [46080/46693 (99%)]\tLoss: 2.363614\n",
      "Test set: Average loss: 2.4035, Accuracy: 1447.0/5189 (27.9%)\n",
      "\n",
      "Train Epoch: 7 [46080/46693 (99%)]\tLoss: 2.506992\n",
      "Test set: Average loss: 2.4038, Accuracy: 1441.0/5189 (27.8%)\n",
      "\n",
      "Train Epoch: 8 [46080/46693 (99%)]\tLoss: 2.380382\n",
      "Test set: Average loss: 2.4003, Accuracy: 1462.0/5189 (28.2%)\n",
      "\n",
      "Train Epoch: 9 [46080/46693 (99%)]\tLoss: 2.377648\n",
      "Test set: Average loss: 2.4011, Accuracy: 1454.0/5189 (28.0%)\n",
      "\n",
      "Train Epoch: 10 [46080/46693 (99%)]\tLoss: 2.429672\n",
      "Test set: Average loss: 2.3986, Accuracy: 1483.0/5189 (28.6%)\n",
      "\n",
      "Train Epoch: 11 [46080/46693 (99%)]\tLoss: 2.398957\n",
      "Test set: Average loss: 2.4005, Accuracy: 1470.0/5189 (28.3%)\n",
      "\n",
      "Train Epoch: 12 [46080/46693 (99%)]\tLoss: 2.442328\n",
      "Test set: Average loss: 2.3923, Accuracy: 1535.0/5189 (29.6%)\n",
      "\n",
      "Train Epoch: 13 [46080/46693 (99%)]\tLoss: 2.405899\n",
      "Test set: Average loss: 2.3960, Accuracy: 1492.0/5189 (28.8%)\n",
      "\n",
      "Train Epoch: 14 [46080/46693 (99%)]\tLoss: 2.447486\n",
      "Test set: Average loss: 2.3953, Accuracy: 1500.0/5189 (28.9%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset1 = MNIST_Polygon2(train_tensor)\n",
    "dataset2 = MNIST_Polygon2(test_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,batch_size=BATCH_SIZE, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "model = Net().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "results_list = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    test(model, DEVICE, test_loader, results_list, epoch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2967b247",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 64\n",
    "EPOCHS = 14\n",
    "LR = 0.002\n",
    "LOG_INTERVAL = 10\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(10, 64, bias=False),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 32, bias=False), \n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, len(y_onehot[0])),\n",
    "            nn.Softmax(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.conv(x)\n",
    "        #x = torch.flatten(x, 1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde75cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()), end=\"\\r\")\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, results_list, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    results_list.append([epoch, correct / len(test_loader.dataset), test_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c1c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = MNIST_Polygon2(train_tensor)\n",
    "dataset2 = MNIST_Polygon2(test_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,batch_size=BATCH_SIZE, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=TEST_BATCH_SIZE, shuffle = True)\n",
    "\n",
    "model = Net().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "results_list = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    test(model, DEVICE, test_loader, results_list, epoch)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517e7f17",
   "metadata": {},
   "source": [
    "# Experiments & Code that doesn't get used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3805662c",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bec35ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X[0].shape\n",
    "print(input_shape)\n",
    "print(len(y_onehot[0]))\n",
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Conv1D(filters=32, kernel_size=(5,), activation='relu', padding='SAME', input_shape=input_shape))\n",
    "model.add(layers.MaxPooling1D(3))\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=(5,), activation='relu', padding='SAME', strides=2))\n",
    "model.add(layers.GlobalAveragePooling1D())\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(len(y_onehot[0]), activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2285005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE = 99\n",
    "EPOCHS = 3\n",
    "\n",
    "history = model.fit(X,\n",
    "                    y_onehot,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7154e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_pand_centrum = create_connection(\"/Users/davemeijdam/Documents/Data Science/Master/Master Thesis/Data/SQLite/Pand_26116_centrum.db\")\n",
    "\n",
    "cur = conn_pand_centrum.cursor()\n",
    "cur.execute(\"SELECT data FROM tiles;\")\n",
    "\n",
    "rows = cur.fetchall()\n",
    "pand_centrum_data = []\n",
    "for row in rows:\n",
    "    pand_centrum_data.append(mapbox_vector_tile.decode(row[0]))\n",
    "    #print(row[0])\n",
    "print(len(pand_centrum_data))\n",
    "\n",
    "## Wegdeel Buiten\n",
    "\n",
    "conn_wegdeel_buiten = create_connection(\"/Users/davemeijdam/Documents/Data Science/Master/Master Thesis/Data/SQLite/Wegdeel_23770_buitengebied.db\")\n",
    "\n",
    "cur = conn_wegdeel_buiten.cursor()\n",
    "cur.execute(\"SELECT data FROM tiles;\")\n",
    "\n",
    "rows = cur.fetchall()\n",
    "wegdeel_buiten_data = []\n",
    "for row in rows:\n",
    "    wegdeel_buiten_data.append(mapbox_vector_tile.decode(row[0]))\n",
    "\n",
    "\n",
    "Lines = []\n",
    "Polygons = []\n",
    "MultiPolygons = []\n",
    "a=0\n",
    "for row in pand_centrum_data[:10000]:\n",
    "    print(str(a) + \" / \" + str(len(pand_centrum_data)), end=\"\\r\")\n",
    "    a = a + 1\n",
    "    keys = row.keys()\n",
    "    \n",
    "    for key in keys:\n",
    "        for element in row[key]['features']:\n",
    "            \n",
    "            if element['geometry']['type'] == 'LineString': \n",
    "                Lines.append(element['geometry']['coordinates'])\n",
    "            \n",
    "            if element['geometry']['type'] == 'Polygon':\n",
    "                Polygons.append(element['geometry']['coordinates'][0])\n",
    "                \n",
    "            #if element['geometry']['type'] == 'MultiPolygon':\n",
    "                #MultiPolygons.append(element['geometry']['coordinates'])\n",
    "    \n",
    "    \n",
    "\n",
    "#test = lvl10_data[0]['spoor.se_fld12_lijngeometrie2d']['features'][0]['geometry']['coordinates']\n",
    "#print(Polygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9e6b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely.geometry as sg\n",
    "import shapely.ops as so\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ls = []\n",
    "#for a in wegdeeljson['features'][:5]:\n",
    "#    ls.append(geometry.Polygon(a['geometry']['coordinates'][0]))\n",
    "\n",
    "new_shape = so.cascaded_union(ls)\n",
    "fig, axs = plt.subplots()\n",
    "axs.set_aspect('equal', 'datalim')\n",
    "\n",
    "for geom in new_shape.geoms:    \n",
    "    xs, ys = geom.exterior.xy    \n",
    "    axs.fill(xs, ys, alpha=1, fc='r', ec='none')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46104a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely.geometry as sg\n",
    "import shapely.ops as so\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "ls = []\n",
    "for element in wegdeel_buiten_data[3]['wegdeel.se_fld15_vlakgeometrie2d']['features']:\n",
    "    \n",
    "    #print(element['geometry']['coordinates'][0])\n",
    "    #geometry.Polygon(element['geometry']['coordinates'][0])\n",
    "    element2 = element['geometry']\n",
    "    \n",
    "    if element2['type'] == 'MultiPolygon':\n",
    "        if element2['coordinates']:\n",
    "            for poly in element2['coordinates'][0]:\n",
    "                print(poly)\n",
    "                ls.append(geometry.Polygon(poly))\n",
    "    \n",
    "    else:\n",
    "        ls.append(geometry.Polygon(element['geometry']['coordinates'][0]))\n",
    "\n",
    "#r1 = sg.Polygon([[243, 2760], [242, 2760], [242, 2761], [243, 2760]])\n",
    "#r2 = sg.Polygon([[243, 2759], [243, 2760], [244, 2760], [244, 2759], [243, 2759]])\n",
    "#r3 = sg.Polygon([[244, 2759], [243, 2759], [243, 2760], [244, 2760], [244, 2759]])\n",
    "#r4 = sg.Polygon([[243, 2759], [242, 2759], [242, 2760], [243, 2760], [243, 2759]])\n",
    "#r5 = sg.Polygon([[241, 2759], [241, 2760], [242, 2759], [241, 2759]])\n",
    "\n",
    "new_shape = so.cascaded_union(ls)\n",
    "fig, axs = plt.subplots()\n",
    "axs.set_aspect('equal', 'datalim')\n",
    "\n",
    "for geom in new_shape.geoms:    \n",
    "    xs, ys = geom.exterior.xy    \n",
    "    axs.fill(xs, ys, alpha=1, fc='r', ec='none')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8546be8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select index of simplification possibility\n",
    "INDEX = 6\n",
    "\n",
    "\n",
    "possibility = simplify_possibilities[INDEX]\n",
    "\n",
    "if possibility[0] == 'D-P':\n",
    "    # Simplification function Douglas-Peucker\n",
    "    simplified_coordinates = simplify_coords(coordinates, possibility[1])\n",
    "\n",
    "if possibility[0] == 'V-W':\n",
    "    # Simplification function Visvalingam-Whyatt\n",
    "    simplified_coordinates = simplify_coords_vw(coordinates, possibility[1])\n",
    "\n",
    "old_xs, old_ys = zip(*coordinates)\n",
    "new_xs, new_ys = zip(*simplified_coordinates)\n",
    "\n",
    "print(len(simplified_coordinates))\n",
    "print(len(coordinates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b3102",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lines = []\n",
    "Polygons = []\n",
    "MultiPolygons = []\n",
    "a=0\n",
    "for row in wegdeel_buiten_data:\n",
    "    print(str(a) + \" / \" + str(len(wegdeel_buiten_data)), end=\"\\r\")\n",
    "    a = a + 1\n",
    "    keys = row.keys()\n",
    "    \n",
    "    for key in keys:\n",
    "        for element in row[key]['features']:\n",
    "            \n",
    "            if element['geometry']['type'] == 'LineString': \n",
    "                Lines.append(element['geometry']['coordinates'])\n",
    "            \n",
    "            if element['geometry']['type'] == 'Polygon':\n",
    "                Polygons.append(element['geometry']['coordinates'][0])\n",
    "                \n",
    "            if element['geometry']['type'] == 'MultiPolygon':\n",
    "                if element['geometry']['coordinates']:\n",
    "                    for poly in element['geometry']['coordinates'][0]:\n",
    "                        MultiPolygons.append(poly)\n",
    "    \n",
    "    \n",
    "\n",
    "#test = lvl10_data[0]['spoor.se_fld12_lijngeometrie2d']['features'][0]['geometry']['coordinates']\n",
    "#print(Polygons)\n",
    "\n",
    "#print(len(Lines))\n",
    "print(len(Polygons))\n",
    "#print(len(MultiPolygons))\n",
    "\n",
    "ls = []\n",
    "for a in Polygons:\n",
    "    ls.append(len(a))\n",
    "    \n",
    "pd.DataFrame({'lengths':Counter(ls).keys(),\n",
    "              'freq':Counter(ls).values()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d093f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "a=0\n",
    "\n",
    "for element in Polygons[:100]:\n",
    "    results_dict = {}\n",
    "    poly1 = geometry.Polygon(element)\n",
    "    results = []\n",
    "    \n",
    "    for possibility in simplify_possibilities:\n",
    "        \n",
    "        if possibility[0] == 'D-P':\n",
    "            # Simplification function Douglas-Peucker\n",
    "            time_start = time()\n",
    "            simplified_coordinates = simplify_coords(element, possibility[1])\n",
    "            time_end = time()\n",
    "            process_time = time_end - time_start\n",
    "\n",
    "        if possibility[0] == 'V-W':\n",
    "            # Simplification function Visvalingam-Whyatt\n",
    "            time_start = time()\n",
    "            simplified_coordinates = simplify_coords_vw(element, possibility[1])\n",
    "            time_end = time()\n",
    "            process_time = time_end - time_start\n",
    "        \n",
    "        \n",
    "        if len(simplified_coordinates) >= 3:\n",
    "            poly2 = geometry.Polygon(simplified_coordinates)\n",
    "            #length_deficit = (poly2.length - poly1.length) / poly1.length\n",
    "        \n",
    "            # If the length deficit of the polygon is smaller(greater) than the provided MAX_LENGTH_DEFICIT, \n",
    "            # the score gets saved\n",
    "            #if length_deficit > MAX_LENGTH_DEFICIT:\n",
    "            \n",
    "            #if length_deficit == 0:\n",
    "            #    score = ScoreFormula(len(element[0]), len(simplified_coordinates), process_time)\n",
    "            #    results.append(score)\n",
    "            #    continue\n",
    "                \n",
    "            #try:\n",
    "            #    if CheckSameIntersections(element[0], simplified_coordinates, grid, ROUNDING) > MIN_INTERSECTIONS_PERC:\n",
    "            #        score = ScoreFormula(len(element[0]), len(simplified_coordinates), process_time)\n",
    "            #        results.append(score)\n",
    "            #except Exception:\n",
    "            #    continue\n",
    "            \n",
    "            if np.isnan(check_pixel_similarity(element, simplified_coordinates, 17)) == True:\n",
    "                results.append('Remove')\n",
    "                break\n",
    "                \n",
    "                \n",
    "            if check_pixel_similarity(element, simplified_coordinates, 17) == 1:\n",
    "                score = ScoreFormula(len(element), len(simplified_coordinates), process_time)\n",
    "                results.append(score)\n",
    "        \n",
    "    results_dict['index'] = a\n",
    "    results_dict['algorithm'] = results.index(max(results))\n",
    "    results_list.append(results_dict)\n",
    "    a = a + 1\n",
    "    \n",
    "results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_INDEX = 85\n",
    "\n",
    "algorithm = simplify_possibilities[results_list[RESULTS_INDEX]['algorithm']]\n",
    "print(algorithm)\n",
    "points = len(Polygons[results_list[RESULTS_INDEX]['index']])\n",
    "o_xs, o_ys = zip(*Polygons[results_list[RESULTS_INDEX]['index']])\n",
    "#geometry.Polygon(Polygons[results_list[RESULTS_INDEX]['index']])\n",
    "\n",
    "if algorithm[0] == 'D-P':\n",
    "    simplified_poly = geometry.Polygon(simplify_coords(Polygons[results_list[RESULTS_INDEX]['index']], algorithm[1]))\n",
    "    simplified_points = len(simplify_coords(Polygons[results_list[RESULTS_INDEX]['index']], algorithm[1]))\n",
    "    s_xs, s_ys = zip(*simplify_coords(Polygons[results_list[RESULTS_INDEX]['index']], algorithm[1]))\n",
    "    \n",
    "if algorithm[0] == 'V-W':\n",
    "    simplified_poly = geometry.Polygon(simplify_coords_vw(Polygons[results_list[RESULTS_INDEX]['index']], algorithm[1]))\n",
    "    simplified_points = len(simplify_coords_vw(Polygons[results_list[RESULTS_INDEX]['index']], algorithm[1]))\n",
    "    s_xs, s_ys = zip(*simplify_coords_vw(Polygons[results_list[RESULTS_INDEX]['index']], algorithm[1]))\n",
    "\n",
    "print(str(simplified_points) + \" / \" + str(points))\n",
    "geometry.Polygon(Polygons[results_list[RESULTS_INDEX]['index']])\n",
    "simplified_poly    \n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(o_xs,o_ys)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(s_xs,s_ys)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be36ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "99*5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacf5798",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(check_pixel_similarity(Polygons[4], Polygons[0], 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3be7f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = geometry.Polygon(Polygons[5])\n",
    "simplified = geometry.Polygon(simplify_coords(Polygons[62],0.5))\n",
    "simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c88a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axis = plt.subplots(3, 3)\n",
    "\n",
    "axis[0,0].imshow(check_pixel_similarity(Polygons[8], Polygons[0], 20))\n",
    "axis[0,1].imshow(check_pixel_similarity(Polygons[2], Polygons[0], 19))\n",
    "axis[0,2].imshow(check_pixel_similarity(Polygons[4], Polygons[0], 17))\n",
    "\n",
    "axis[1,0].imshow(check_pixel_similarity(Polygons[10], Polygons[0], 19))\n",
    "axis[1,1].imshow(check_pixel_similarity(Polygons[12], Polygons[0], 18))\n",
    "axis[1,2].imshow(check_pixel_similarity(Polygons[16], Polygons[0], 18))\n",
    "\n",
    "axis[2,0].imshow(check_pixel_similarity(Polygons[32], Polygons[0], 19))\n",
    "axis[2,1].imshow(check_pixel_similarity(Polygons[26], Polygons[0], 18))\n",
    "axis[2,2].imshow(check_pixel_similarity(Polygons[24], Polygons[0], 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c218b87a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
